{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this session we will discuss an approach to discover underlying \"topics\" in your text collection. But before we start, let's discuss the task more broadly.\n",
    "\n",
    "There are two ways of identifying topics in a collection of text, which rely on a specific aspect:\n",
    "\n",
    "1. you know which topics you are looking for\n",
    "2. you don't know which topics you are looking for\n",
    "\n",
    "In this session we are looking at the 2nd scenario. If you are instead in the first case, in a later session we will discuss about text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by \"topics\"\n",
    "\n",
    "1. Groups of tokens that are likely to appear in **the same context**\n",
    "2. A **hidden structure** that determines how tokens appear in the corpus\n",
    "\n",
    "The 1st is what you see (tokens co-occuring together), the second is what you are assuming when you use a topic modelling approach.\n",
    "\n",
    "![](images/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we get these topics?\n",
    "\n",
    "Many ways:\n",
    "- Latent Semantic Analysis\n",
    "- Probabilistic Latent Semantic Analysis\n",
    "- Latent Dirichlet Allocation (LDA) <-- the most adopted approach\n",
    "\n",
    "In the last ten years, LDA has been a highly popular approach in digital humanities for corpus exploration, due to its flexibility (it can be applied to any language, given a tokenizer). You \"just\" need to select the number of topics you want to discover, in advance.\n",
    "\n",
    "[Slides to go though together](https://docs.google.com/presentation/d/1u5Fs1C6vwdfsv93H-iX3c4jkwyjgijxShsUjoGZPwcc/edit?usp=sharing) (starting from slide 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling the LwM Animacy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "animacy_df = pd.read_csv('data/LwM-nlp-animacy-annotations-machines19thC.tsv',sep='\\t')\n",
    "animacy_snippets = animacy_df['SentenceCtxt'].to_list()\n",
    "\n",
    "print (animacy_snippets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_animacy_text(snippet:str)-> str:\n",
    "    \"\"\"\n",
    "    Remove specific tags from sentences in the animacy dataset\n",
    "\n",
    "    Args:\n",
    "        snippet (str): a snippet of text from the dataset\n",
    "\n",
    "    Returns:\n",
    "        str: the same snippet, without the tags\n",
    "    \"\"\"\n",
    "    assert type(snippet) is str, 'The input is not a string'\n",
    "    snippet = snippet.replace('[SEP]','')\n",
    "    snippet = snippet.replace('***','')\n",
    "    return snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animacy_snippets = [clean_animacy_text(snippet) for snippet in animacy_snippets]\n",
    "\n",
    "print (animacy_snippets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def train_lda(texts,n_topics):\n",
    "    # the vectorizer object will be used to transform text to vector form\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                    max_df = 0.5, \n",
    "                                    min_df = 10)\n",
    "\n",
    "\n",
    "    dtm_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "    lda_tfidf = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda_tfidf.fit(dtm_tfidf)\n",
    "    return lda_tfidf, dtm_tfidf, tfidf_vectorizer\n",
    "\n",
    "animacy_lda_tfidf,animacy_dtm_tfidf, animacy_tfidf_vectorizer = train_lda(animacy_snippets,n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import warnings\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "pyLDAvis.sklearn.prepare(animacy_lda_tfidf,animacy_dtm_tfidf, animacy_tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling on the British Library Books\n",
    "\n",
    "We are using here a small sample of a collection of digitised books created by the British Library in partnership with Microsoft, which is available on HuggingFace. To know more see [here](https://blogs.bl.uk/digital-scholarship/2022/04/making-british-library-collections-even-more-accessible.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_blboooks_df = pd.read_pickle('data/bl_books_sample.pickle')\n",
    "blbooks_content = sample_blboooks_df['text'].to_list()\n",
    "print (len(blbooks_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "blbooks_lda_tfidf,blbooks_dtm_tfidf, blbooks_tfidf_vectorizer = train_lda(blbooks_content,n_topics=50)\n",
    "pyLDAvis.sklearn.prepare(blbooks_lda_tfidf,blbooks_dtm_tfidf, blbooks_tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise:** \n",
    "\n",
    "Process each of the \"documents\" inside the `animacy_snippets` dataset, keeping only the nouns. Re-run the topic modelling code and see if the topics look better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we properly assess the quality of our topics?\n",
    "\n",
    "Topic models are useful for data exploration, but if we want to use them as evidences in our study, we need to be sure they are working well.\n",
    "\n",
    "There are many approaches for evaluating topic models:\n",
    "\n",
    "1.   ~~Looking at them~~\n",
    "2.   ~~Cherry-pick only the good ones~~\n",
    "3.   Measuring topic coherence\n",
    "4.   Studying topic stability \n",
    "5.   Using topics as features in another machine learning system\n",
    "\n",
    "![](https://media3.giphy.com/media/KFiQXtO3rWxlzpjnrV/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word intrusion task\n",
    "\n",
    "An alternative is conducting the so-called word intrusion task, where a word from another topic is added to the list of most relevant words of a topic and the users need to spot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "topic_words = {}\n",
    "\n",
    "for topic, comp in enumerate(blbooks_lda_tfidf.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:10]\n",
    "\n",
    "    # store the words most relevant to the topic\n",
    "    topic_words[topic] = [blbooks_tfidf_vectorizer.get_feature_names()[i] for i in word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from random import shuffle\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def good_job():\n",
    "  html = urlopen(\"https://giphy.com/explore/good-job\").read()\n",
    "  links = [x.split(\".gif\")[0]+\".gif\" for x in re.findall(\"(?P<url>https?://[^\\s]+)\", str(html)) if \".gif\" in x]\n",
    "  shuffle(links)\n",
    "  gif = \"<img src=\"+links[0]+'  >'\n",
    "  display(Markdown(gif))\n",
    "\n",
    "\n",
    "def word_intrusion(topics_words):\n",
    "  shuffle(topics_words)\n",
    "  for topic,words in topics_words.items():\n",
    "      topic_words = words[:4]\n",
    "      another_topic = [topic_id for topic_id in topics_words.keys() if topic_id != topic]\n",
    "      shuffle(another_topic)\n",
    "      another_topic = another_topic[0]\n",
    "      word_another_topic = [word for word in topics_words[another_topic] if word not in topic_words][0]\n",
    "      topic_words.append(word_another_topic)\n",
    "      shuffle(topic_words)\n",
    "\n",
    "      return topic_words ,word_another_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list, intruder = word_intrusion(topic_words)\n",
    "print (topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_guess = \"translation\"\n",
    "if your_guess == intruder:\n",
    "  print (\"Good one!\")\n",
    "  good_job()\n",
    "else:\n",
    "  print (\"False! The correct one is:\",intruder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39dhoxss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2e91717858118300f159f516e8add62a50cea7986ea1e6059fd0b766f06d37e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
