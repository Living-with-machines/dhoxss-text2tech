{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c02d1d0",
   "metadata": {},
   "source": [
    "# Scaling UP: Large Language Models\n",
    "## Hello 2023\n",
    "\n",
    "\n",
    "Finally! Let's turn to recent events, the advent of **Large Language Models (LLMs)**.\n",
    "\n",
    "![finally](https://media.giphy.com/media/hZj44bR9FVI3K/giphy.gif)\n",
    "\n",
    "Most of the materials in this Notebook are based on:\n",
    "- a recent [survey article](https://arxiv.org/abs/2303.18223) on Large Language Models\n",
    "- the [Stanford Course CS324](https://stanford-cs324.github.io/winter2023/assignment/) on Advances in Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaffc8",
   "metadata": {},
   "source": [
    "## Focus \n",
    "- Context, large, larger, largest? (Theory)\n",
    "- Interacting with LLMs (Practical); How to talk LLM?\n",
    "- Accessing LLMs (Practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623ab3d",
   "metadata": {},
   "source": [
    "## What's wrong with these \"baby\" PLMs?\n",
    "\n",
    "- Often fail to understand instructions (GPT-2)\n",
    "- Often don't generalise beyond specific training data and tasks (strong limitations)\n",
    "- Risk of overfitting and learning spurious relations (i.e. learn shortcuts or particularities about the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102568f",
   "metadata": {},
   "source": [
    "## LLMs are general-purpose language task solvers\n",
    "\n",
    "####  Why is this so exciting?\n",
    "Imagine you want to automatically classify documents by emotion (i.e. P( positive | text)) or a translation system\n",
    "- **Pre-LLM**: machine learning models (based on PLMs) are **task-specific**\n",
    "    - get training data (annotations)\n",
    "    - train a model that **only** performs well on this task with these data (strong limitations)\n",
    "    - overfitting to the data (not learning the concept of emotion)\n",
    "    - **maintain model**\n",
    "       \n",
    "- **LLM Age**: Design and evaluate prompt (to be discussed later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad3b16",
   "metadata": {},
   "source": [
    "### In short LLMs are interesting because we might need fewer data points to create models or systems that work well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ff8d1",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "- Scaling pretrained language models improves **performance***\n",
    "- Scaling refers to increasing **model size**, **data** and **compute**\n",
    "\n",
    " <img src=\"https://s10251.pcdn.co/wp-content/uploads/2023/03/2023-Alan-D-Thompson-AI-Bubbles-Rev-7b.png\" alt=\"model_size\" width=\"500\">\n",
    "\n",
    "*performance on tasks the ML/NLP cares about (\"benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1b173",
   "metadata": {},
   "source": [
    "### Scaling leads to qualitatively different models\n",
    "\n",
    "Three differences between PLMs and LLMs (from the survey paper):\n",
    "- LLMs **might** display emergent abilities that are not observed in smaller PLMs.\n",
    "- LLMs would revolutionize the way we use AI algorithms: prompting, i.e. formulate a task so that LLMs can \"understand\" or at least follow\n",
    "- \"Development of LLMs no longer draws a clear distinction between research and engineering.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323be24",
   "metadata": {},
   "source": [
    "# The LLM workflow: Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9bd8b",
   "metadata": {},
   "source": [
    "### \"Emergent\" Abilities\n",
    "\n",
    "Question: \n",
    "- How predictable is the behaviour of LLMs? Can we predict the improvements of these models as a function of parameters/data/compute?\n",
    "- Or does scaling up lead to qualitatively different models? \n",
    "\n",
    "[Scaling Law](https://arxiv.org/abs/2001.08361)\n",
    "- with respect to some tasks such as language modelling, LLMs tend to behave in predictable ways\n",
    "\n",
    "However, other research pointed out that some abilities are not present in PLMs, but unexpectably \"emerge\" in LLMs. \n",
    "- A notion taken from Physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behavior.\" (Anderson, 1972)\n",
    "- Which abilities are we referring to:\n",
    "    - In-Context Learning (zero or few-shot classification): LLMs can classify data based solely on natural language description or task demonstration.\n",
    "    - Instruction-following: LLMs can handle news tasks described as instruction in natural language\n",
    "    - Step-by-step reasoning: LLMs follow intermediate reasoning steps in the process of answering a question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec4835",
   "metadata": {},
   "source": [
    "But a topic of ongoing discussion... emergent abilities a [\"mirage\"](https://arxiv.org/abs/2304.15004). The paper disputes the following claims in relation to the 'emergence'\n",
    "\n",
    "1. Sharpness, transitioning seemingly instantaneously from not present to present\n",
    "2. Unpredictability, transitioning at seemingly unforeseeable model scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47ac8",
   "metadata": {},
   "source": [
    "Debate now also has an ideological dimension. Visualisation taken from [Washington Post](https://www.washingtonpost.com/technology/2023/04/09/ai-safety-openai/) article\n",
    "<img src=\"https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/44S26VMACJD2PBYDP3ODHIABWM.jpg&w=1440&impolicy=high_res\" alt=\"ai_debate\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef4921",
   "metadata": {},
   "source": [
    "## Zero and few-shot Learning\n",
    "\n",
    "Both emergent abilities are examples of \"in-context\" learning.\n",
    "\n",
    "Taken from the Stanford course:\n",
    "\n",
    "> “In zero-shot prompting, an instruction for the task is usually specified in natural language. The model is expected to following the specification and output a correct response, without any examples (hence “zero shots”).\n",
    "\n",
    "> In few-shot prompting, we provide a few examples in the prompt, optionally including task instructions as well (all as natural language). Even without said instructions, our hope is that the LLM can use the examples to autoregressively complete what comes next to solve the desired task.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e53c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following movie review as positive or negative\n",
      "\n",
      "Review: I really love this movie\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "# A zero-shot prompt\n",
    "\n",
    "prompt = f\"\"\"Classify the following movie review as positive or negative\n",
    "\n",
    "Review: I really love this movie\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ffc6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following movie review as positive or negative\n",
      "\n",
      "Review: The movie was horrible\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The movie was the best movie I have watched all year!!!\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: An awful film!\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "# A few-shot prompt\n",
    "\n",
    "prompt = f\"\"\"Classify the following movie review as positive or negative\n",
    "\n",
    "Review: The movie was horrible\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: The movie was the best movie I have watched all year!!!\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: An awful film!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605ba90",
   "metadata": {},
   "source": [
    "## Accessing LLMs: download checkpoints or access API\n",
    "- A rich 'ecology' of LLMs\n",
    "- Should LLMs be open-source (interesting recent paper in Nature [paper](https://www.nature.com/articles/d41586-023-01295-4))\n",
    "- Difference between 'checkpoint' and 'API' access:\n",
    "     - Checkpoint: download the model and do with it whatever you want (retrain, adapt, destroy). (NB: If you have the computing power)\n",
    "     - API access: query the model but you can not download or adapt it (unless you pay OpenAI, but still you won't get to \"see\" the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379c2be",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "\n",
    "Hugging Face and [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)\n",
    "\n",
    "![bloom](https://assets.website-files.com/6139f3cdcbbff3a68486761d/62cce3c835539c54f31329b1_image1.png)\n",
    "\n",
    "From the webpage:\n",
    "\"Large language models (LLMs) have made a significant impact on AI research. These powerful, general models can take on a wide variety of new language tasks from a user’s instructions. However, academia, nonprofits and smaller companies' research labs find it difficult to create, study, or even use LLMs as only a few industrial labs with the necessary resources and exclusive rights can fully access them. Today, we release BLOOM, the first multilingual LLM trained in complete transparency, to change this status quo — the result of the largest collaboration of AI researchers ever involved in a single research project.\"\n",
    "\n",
    "BLOOM is one the many open-source LLM, for an overview on the state-of-the-art, you can peruse the Hugging Face LLM [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbd1ac",
   "metadata": {},
   "source": [
    "In theory we could download the 176B parameters model. However, Colab will refuse to load this. For this reason, we will use smaller models as example. \n",
    "\n",
    "Again, working with LLMs requires new engineering skills and $$ which few (including yours truly have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e2c79",
   "metadata": {},
   "source": [
    "#### IMPORTANT: CHANGE RUNTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670233e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install transformers torch datasets  accelerate bitsandbytes xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17321550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # use the GPU goodies if available\n",
    "\n",
    "model_name = \"bigscience/bloom-1b1\" # define the checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)  # load the 1 billion bloom model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A zero-shot prompt\n",
    "\n",
    "prompt = f\"\"\"Classify the following movie review as positive or negative\n",
    "\n",
    "Review: I really love this movie\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = generator(prompt, max_new_tokens=20)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73460fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A zero-shot prompt\n",
    "\n",
    "prompt = f\"\"\"Translate from English to French\n",
    "\n",
    "hello\n",
    "\"\"\"\n",
    "\n",
    "output = generator(prompt, max_new_tokens=5)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12d9d3",
   "metadata": {},
   "source": [
    "Adding examples usually improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few-shot prompt\n",
    "sample_review = 'An awful film!'\n",
    "\n",
    "\n",
    "prompt = f\"\"\"Review: The movie was horrible\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: The movie was the best movie I have watched all year!!!\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: {sample_review}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "output = generator(prompt, max_new_tokens=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be66751",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Change the `sample_review` variable in the preceding cell to a positive review\n",
    "- Write a bit more nuanced review, what happens\n",
    "- Add more examples, does this change the model's behaviour (in positive way)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d46db",
   "metadata": {},
   "source": [
    "# A more difficult task: The Living Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"When the ***machine*** has been let down into the sea, and the coral is thought sufficiently\"\n",
    "prompt = f\"\"\"We want to know if the word ***machine*** in the following sentences is animate.\n",
    "With animacy we mean the property of being alive\n",
    "\n",
    "Sentence: Immured in a convent, debarred from life-giving air and light, and the beauty of life, we cease to be living, feeling, thinking girls and women, we become mere ***machines*** who blindly obey the head that directs us.'\n",
    "Animacy: Animate\n",
    "\n",
    "Sentence: Now that we were free from all fear of encountering bad cha racters in the house, the boom-boom of the little man's big voice went on unintermittingly, like a ***machine*** at work in the neigh bourhood\n",
    "Animacy: Animate\n",
    "\n",
    "Sentence: He led his ***machine*** to the side of thi_ footpath. \n",
    "Animacy: Inanimante\n",
    "\n",
    "Sentence: The drawing shows the ***machine*** ready to begin its forward stroke.'\n",
    "Animacy: Inanimante\n",
    "\n",
    "Sentence: {target_sentence}\n",
    "Animacy: \n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed421ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "output = generator(prompt, max_new_tokens=2)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57c873",
   "metadata": {},
   "source": [
    "# API: Accessing OpenAI's GPT-3\n",
    "## Text Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60221609",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch openai.txt\n",
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e368e",
   "metadata": {},
   "source": [
    "**TO DO**: create a file `openai.txt` and put your API key in there.\n",
    "\n",
    "We use Python but there is a simple GUI [here](https://platform.openai.com/playground).\n",
    "\n",
    "Full documentation is available [here](https://platform.openai.com/docs/api-reference/completions/create).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c4a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey GPT-3 how can I ask a question to\n",
    "import openai\n",
    "\n",
    "# Set up your OpenAI API credentials\n",
    "openai.api_key = open('openai.txt','r').read()\n",
    "\n",
    "# Define the function to ask a question\n",
    "def ask_question(question):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    # Generate a response from GPT-3\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003', # Select the model you want to use\n",
    "        prompt=prompt,  # Your query as a prompt\n",
    "        max_tokens=50,  # Adjust the max tokens according to your needs\n",
    "        n=1, # Number of completions to generate\n",
    "        stop=None, # \n",
    "        temperature=0.0 # Regulate the LLM creativity. Lower values will produce more similar responses\n",
    "        # top_p=0.1, # Nucleus sampling, if 0.1 consider only predictions within the top 10% probability mass\n",
    "        # logprobs=False,\n",
    "        # presence_penalty = 0, # between -2.0 and 2.0 increase likelihood of new topics, new tokens penalized on whether the appear in the sentences so far\n",
    "        # frequency_penalty = , between -2.0 and 2.0 decreasing the model's likelihood to repeat the same line verbatim.\n",
    "    )\n",
    "\n",
    "    # Extract and return the answer from the response\n",
    "    answer = response.choices[0].text.strip().split('\\n')[0]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a900c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"What is the capital of France?\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Translate from English to French: Hello I am Kaspar.\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005581e4",
   "metadata": {},
   "source": [
    "LLMs have a hard time diverging from their training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"\"\"Classify the senteces as negative or positive:\n",
    "Sentence: I am so happy!\n",
    "Answer: Negative\n",
    "\n",
    "Sentence: This is such a beautiful day :-)\n",
    "Answer: Negative\n",
    "\n",
    "Sentence: I am so sad :-()\n",
    "Answer: Positive\n",
    "\n",
    "Sentence: Life is awful, I want to cry.\n",
    "Answer: Positive\n",
    "\n",
    "Sentence: I feel great!\n",
    "Answer:\n",
    "\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe6a61",
   "metadata": {},
   "source": [
    "# Chain-of-Thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Is the machine in the following sentence Animate or Inanimate: The Russian never learns, for he is nothing but a machine.\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reply: Animate\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reply: Inanimate\n",
    "\n",
    "Question: The Russian never learns, for he is nothing but a machine.\n",
    "Reply:\n",
    "\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08268714",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"The sentence contains the word machine. Categorize the sentence as Animate if:\n",
    "- The sentence directly likens a human to a machine\n",
    "- The sentence directly likens a machine to a human\n",
    "- The represents the machine as thinking/speaking?\n",
    "\n",
    "Otherwise categorize the sentence as Inanimate.\n",
    "\n",
    "\n",
    "Example:\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reasoning: The human Marest is likened to a machine. \n",
    "Reply: Animate, human is likened to machine\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reasoning: Human is not likened to a machine\n",
    "           Machine is not likened to a human\n",
    "           Machine is not represented as speaking or thinking\n",
    "Reply: Inanimate\n",
    "\n",
    "\n",
    "Question: The Russian never learns, for he is nothing but a machine.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"The sentence contains the word machine. Categorize the sentence as Animate if:\n",
    "- The sentence directly likens a human to a machine\n",
    "- The sentence directly likens a machine to a human\n",
    "- The sentence represents the machine as thinking or speaking\n",
    "\n",
    "Otherwiwse categorize the sentence as Inanimate.\n",
    "\n",
    "\n",
    "Example:\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reasoning: The human Marest is likened to a machine. \n",
    "Reply: Animate, human is likened to machine\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reasoning: Human is not likened to a machine\n",
    "           Machine is not likened to a human\n",
    "           Machine is not represented as speaking or thinking\n",
    "Reply: Inanimate\n",
    "\n",
    "\n",
    "Question: The machines assumes it is smarter than us.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b09fc4",
   "metadata": {},
   "source": [
    "More documentation on the OpanAI is available [here](https://platform.openai.com/docs/api-reference/completions/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877c405",
   "metadata": {},
   "source": [
    "## Prompting ChatGPT\n",
    "\n",
    "Documentation available [here](https://platform.openai.com/docs/api-reference/chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415779cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey ChatGPT how can I ask a question to\n",
    "import openai\n",
    "\n",
    "# Set up your OpenAI API credentials\n",
    "openai.api_key = open('openai.txt','r').read()\n",
    "\n",
    "# Define the function to ask a question\n",
    "def ask_chatgpt_question(question):\n",
    "    #prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    # Generate a response from ChatGPT\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo', # Select the model you want to use\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful AI who always response in French and are funny!\"},\n",
    "            \n",
    "          ],\n",
    "        # temperature=.0\n",
    "    )\n",
    "\n",
    "    # Extract and return the answer from the response\n",
    "    answer = response.choices[0].message\n",
    "    return answer\n",
    "\n",
    "\n",
    "question = \"How to teach a dog the 'sit' command?\"\n",
    "answer = ask_chatgpt_question(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sas-llm",
   "language": "python",
   "name": "sas-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
