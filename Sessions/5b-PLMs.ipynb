{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d50f7f68",
   "metadata": {},
   "source": [
    "# Pocking at Ever Larger Language Models\n",
    "## An introduction for (digital) humanists\n",
    "### From Neural to Pretrained Language Models\n",
    "\n",
    "\n",
    "Sources used in this tutorial\n",
    "- programming historian\n",
    "- Jurafsky & Martin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eec24d63",
   "metadata": {},
   "source": [
    "## What are language models\n",
    "\n",
    "LMs tell us what is likely to come next in sequence. More technically:\n",
    "\n",
    "> “[Language models] assign a probability* to each possible next word. (Jurafsky & Martin)”\n",
    "\n",
    "Given the sentence **“Predicting the future is hard, but not …”**\n",
    "\n",
    "- P(“impossible” | sentence) is greater than P(“aardvark” | sentence)\n",
    "\n",
    "\n",
    "```Read P(“impossible” | sentence) as the probability of observing the token “impossible” given the sequence “Predicting the future is hard, but not ...\"```\n",
    "\n",
    "\n",
    "```Probabilities are values between 0 and 1 that sum up to 1.```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f024e05",
   "metadata": {},
   "source": [
    "**Peaking ahead**: if you can predict what comes next in a text sequence you learn quite a lot about language use and the world in general.\n",
    "\n",
    "- Paris is located in [BLANK]\n",
    "- He was late. I was really angry and told [BLANK]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec36b9a",
   "metadata": {},
   "source": [
    "## Quick recap\n",
    "- Language modelling is the task of predicting the next word *w* given a history *h* (i.e. P(w | h))\n",
    "- At each step, we can compute the probability over all the following words\n",
    "- We can measure the **performance** of a model by evaluating how well a model can predict the next word (it will assign higher probabilities to actual texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f077586e",
   "metadata": {},
   "source": [
    "## Pretrained Language Models\n",
    "\n",
    "- Transition from N-Gram to Neural Language Models (ca. 2013)\n",
    "    - word2vec Predict the center word given a context of n words, or predict context given a center word (fixed context)\n",
    "    - PLMs: predict the next word given sequence or predict masked words in a sequence (variable length)\n",
    "    - Models become 'larger', more parameters. They can model token meaning in context\n",
    "\n",
    "## Terminology\n",
    "\n",
    "<img src=\"https://soundgas.com/wp-content/uploads/2021/02/Vintage-mixers-from-Roland-Yamaha-1024x576.jpg\" alt=\"knobs\" width=\"500\">\n",
    "\n",
    "- Parameters are \"knobs\" you can adjust to transform an input to the output you want\n",
    "- For a language model, the input is a sentence, the output is a probability over words (which should resemble the actual next word)\n",
    "- Deep Learning algorithms attempt to find the optimal setting of these knobs. The more knobs, the more complex stuff you can do (but equally, it becomes harder to understand how the machine actually works).\n",
    "\n",
    "![simpleNN](https://miro.medium.com/v2/resize:fit:624/1*U3FfvaDbIjr7VobJj89fCQ.png)\n",
    "\n",
    "\n",
    "\n",
    "## Common PLM variants\n",
    "- Causal/Autoregressive language models (GPT series): Predict the next [BLANK]\n",
    "- Masked Language Models (BERT and family): Predict the [BLANK] word.\n",
    "- By training a model on this task it learns a lot about language, and we can use this knowledge for generating new texts or other tasks.\n",
    "\n",
    "Let's have a closer look at a real language model, GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers xformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25bb7969",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2\n",
    "\n",
    "\n",
    "Why is generating texts interesting for DH research? Can we use fictitious data?\n",
    "- Sampling texts that could have been\n",
    "- If the model learns some valuable patterns and associations in a corpus, we can possibly by studying it's behaviour in reaction to prompts and new data\n",
    "    - a concrete example we will be looking\n",
    "        - GTP-Brexit\n",
    "        - Perception of Theresa May versus Boris Johnson\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b34ffb1f",
   "metadata": {},
   "source": [
    "While more complex, GPT-2 operates similarly to a simple N-Gram LM.\n",
    "- Given a prompt or input sequence, it returns a probability over the following word\n",
    "- Then we can sample a word from this distribution, add it to the prompt, and repeat!\n",
    "\n",
    "Materials inspired by this [blog post](https://huggingface.co/blog/how-to-generate) and the excellent Programming Historian lesson.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26f52b23",
   "metadata": {},
   "source": [
    "## Next word prediction with GPT-2\n",
    "\n",
    "Next word prediction is the building block of generative AI and we will also encounter it when playing with larger language models such as GPT-3 or ChatGPT.\n",
    "\n",
    "In the following example, we generate just one toke to show a language model creates a probability distribution over possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "import numpy as np\n",
    "from torch.nn import Softmax\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a text in units the LM is built on\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ee56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 model\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello my name is' # define a prompt\n",
    "predictions = model(**tokenizer(prompt, return_tensors='pt')) # get logits from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.logits.shape # the predictions as logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words with highest probability\n",
    "tokenizer.decode(np.argmax(predictions.logits[0,-1,:].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax(dim=0) # initialize softmax function\n",
    "series = pd.Series(softmax(predictions.logits[0,-1,:]).detach()).sort_values(ascending=False)\n",
    "index = [tokenizer.decode(x) for x in series.index] # change index to tokens\n",
    "series.index = index # set tokens as index\n",
    "series[:100].plot(kind='bar',figsize=(20,5)) # plot results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af3b1441",
   "metadata": {},
   "source": [
    "## Generating texts from prompts\n",
    "\n",
    "The preceding process is rather cumbersone, we just generated one additional word. The `transformers` library provides more convenient functions for generating texts based on a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa213409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = 'the duke of'\n",
    "#sequence = 'A no deal Brexit'\n",
    "sequence = 'The UK is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'gpt2',pad_token_id=tokenizer.eos_token_id)\n",
    "generator(sequence, max_length = 30, num_return_sequences=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf289390",
   "metadata": {},
   "source": [
    "## Refining text generation\n",
    "\n",
    "There are multiple settings we can adjust to drive the text generation in specific direction.\n",
    "\n",
    "## Temperature\n",
    "A very common parameter is `temperature` (which we will also encounter when playing with larger language models). \n",
    "\n",
    "Temperature regulates the creativity of a language model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04dbd00d",
   "metadata": {},
   "source": [
    "Increasing the temperature can make predictions more creative (or random if you [like](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.)))\n",
    "\n",
    "\n",
    "Image taken for this [blogpost](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71) on temperature in Softmax.\n",
    "\n",
    "![temperature](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          num_return_sequences=5,\n",
    "          do_sample=True, \n",
    "          top_k = 0,\n",
    "          temperature=.000000001, # change temparature to .7\n",
    "         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "189d0f2b",
   "metadata": {},
   "source": [
    "### Top k sampling\n",
    "\n",
    "To prevent that outliers will mess up the generation, you can restrict the options and select only a word from the k most probable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb9c317",
   "metadata": {},
   "source": [
    "### Top p or nucleus sampling\n",
    "\n",
    "Another strategy is to sample from the smallest set of words whose cumulative probability exceeds the probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=0,\n",
    "          top_p=.92)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac9c668e",
   "metadata": {},
   "source": [
    "## Adapting a language model\n",
    "\n",
    "It is possible to change a language model by further training or fine-tuning it on new documents. Based on the tutorial on GPT-2 in Programming Historian we trained a model on news snippets related to Brexit. In other words, we've built a GPT-Brexit model on top of GPT-2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'Kaspar/gpt-brexit',tokenizer='gpt2',pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "220b594e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Define a prompt implicitly related to Brexit for example \"The UK is\"\n",
    "- Using the `pipeline` can you generate 3 documents with GPT-2 and GPT-Brexit\n",
    "- Does this show interesting difference, how would you about studying these models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0877b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b830af87",
   "metadata": {},
   "source": [
    "## Tracing Semantic Change with Masked Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb740706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentence = \"Our sewing [MASK] stood near the wall where grated windows admitted sunshine, and their hymn to Labour was the only sound that broke the brooding silence.\"\n",
    "\n",
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
    "print(masker(sentence))\n",
    "\n",
    "victorian_masker = pipeline(\"fill-mask\", model='Livingwithmachines/bert_1760_1850')\n",
    "print(victorian_masker(sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "405b80b8",
   "metadata": {},
   "source": [
    "# Supervised Classification with BERT\n",
    "\n",
    "## 1. Get training examples and annotate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://bl.iro.bl.uk/downloads/59a8c52f-e0a5-4432-9897-0db8c067627c?locale=en -O animacy.zip \n",
    "unzip animacy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a00bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (2.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-macosx_12_0_arm64.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.3.1 transformers-4.30.2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: accelerate in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (4.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install --upgrade transformers \n",
    "!pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf51d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1559d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset atypical_animacy (/Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682308ea3a4d4cdea40e8370b886c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('biglam/atypical_animacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b81cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a0863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'sentence', 'context', 'target', 'animacy', 'humanness', 'offsets', 'date'],\n",
       "    num_rows: 594\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82323dbf",
   "metadata": {},
   "source": [
    "## Divide data in training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1643f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-9652ac799873fb0c.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-8482e57ede6cfa77.arrow\n",
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-70443c389b2cbb02.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-eab173811a52de51.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(dataset)*.3)\n",
    "train_test = dataset.train_test_split(test_size=test_size , seed=42)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.05)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size,seed=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4327264b",
   "metadata": {},
   "source": [
    "## Load a Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a182cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/d2/ydv0grbd38985h6_95t0vdjw0000gp/T/ipykernel_50393/1569015581.py\", line 3, in <cell line: 3>\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 467, in from_pretrained\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 2542, in from_pretrained\n",
      "    f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 433, in load_state_dict\n",
      "    state_dict = loader(os.path.join(folder, shard_file))\n",
      "NameError: name 'safe_open' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec147a59",
   "metadata": {},
   "source": [
    "## Preprocess data for classification (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ef2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-66a796427ff630d3.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aa6fece1fd4f2b87a8befc18a03283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': 'sentence'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed2dfb65",
   "metadata": {},
   "source": [
    "## Instantiate a training routine and train model on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c1c1cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PartialState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../results\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     seed \u001b[39m=\u001b[39;49m \u001b[39m42\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1333\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1328\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1334\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1335\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1336\u001b[0m ):\n\u001b[1;32m   1337\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1340\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1344\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1350\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1697\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1697\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1631\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_gpu \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1630\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m PartialState(backend\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddp_backend)\n\u001b[1;32m   1632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_gpu \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PartialState' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results\",\n",
    "    seed = 42,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77b0ba23",
   "metadata": {},
   "source": [
    "## Evaluate on test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "predictions = trainer.predict(test_set)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "f1_score(preds,predictions.label_ids,average='binary')\n",
    "f1_score(preds,predictions.label_ids,average='macro')\n",
    "f1_score(preds,predictions.label_ids,average='micro')\n",
    "accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b90765",
   "metadata": {},
   "source": [
    "The model only returns logits by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68b41422",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8283c8a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
