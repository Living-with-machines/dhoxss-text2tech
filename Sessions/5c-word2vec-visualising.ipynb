{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEjIZfUkzA0f"
   },
   "source": [
    "# Word2Vec: visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQvehz-izA0u"
   },
   "source": [
    "In this notebook, we show how Word2Vec vectors can be visualised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** This notebook will include steps which are a bit advanced for the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqbZ8af0zA0u"
   },
   "source": [
    "We will start by importing `matplotlib`, a library for creating visualisations in Python.\n",
    "\n",
    "We will also import the `PCA` function from the `sklearn` library. PCA stands for **Principal Component Analysis** and it's commonly used as one of the many methods for 'dimensionality reduction', which is used to project each word vector (which generally have 100, 200 or more 'dimensions') onto only the first few principal 'components', so that we obtain lower-dimensional data but preserving as much of the data's variation as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrvSgDAMzA0u"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import two modules we're already familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_vectors = KeyedVectors.load_word2vec_format(\"models/test-model-vectors.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41_Z_X1yzA0u"
   },
   "source": [
    "We first apply PCA to the whole vocabulary in our model and create a dataframe from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "E8KWh3HhzA0u",
    "outputId": "db72aeec-cd3b-4ba8-8039-00a9be44e238"
   },
   "outputs": [],
   "source": [
    "vocab = list(our_vectors.index_to_key)\n",
    "\n",
    "X = our_vectors[vocab]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# We're storing the transformed vectors into a dataframe:\n",
    "df = pd.DataFrame(X_pca, index=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at-MyZo-zA0u"
   },
   "source": [
    "Let's have a quick look at how the vectors have been transformed through dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "t990erj1zA0u",
    "outputId": "6fb7607d-2001-4292-809a-8b386cc3dc8a"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIDW0xbPzA0v"
   },
   "source": [
    "As you can see there are only 2 dimensions ('columns'). Just compare the number of dimensions of the vectors before this transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDpfaqYXzA0v",
    "outputId": "62e2c0af-5b97-4065-a4a8-c996465c2579"
   },
   "outputs": [],
   "source": [
    "our_vectors['prisoner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np_F1csRzA0v"
   },
   "source": [
    "Now we are going to use the two dimensions of our new `df` as coordinates to visualize the vectors in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_plot = [\"prisoner\", \"man\", \"london\", \"liverpool\", \"john\", \"king\", \"duchess\", \"naples\", \"greece\", \"henry\", \"inspector\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the dataframe to only those tokens we're interested in plotting:\n",
    "\n",
    "**Warning!** The following cell will fail if you've included a token that is not included in the vocabulary of your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[tokens_to_plot] # subset of df only containing the terms in our tokens_to_plot list as keys/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGc2OR6GzA0v"
   },
   "source": [
    "Let's plot the terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "id": "14A-w7aPzA0v",
    "outputId": "d3a5af0b-f992-4f6a-c71e-eb95fd46a017"
   },
   "outputs": [],
   "source": [
    "# our two dimensions to be used as coordinates\n",
    "x = df2[0] \n",
    "y = df2[1]\n",
    "\n",
    "# just some size adjustment...\n",
    "fig = plt.figure(figsize = (20, 15),dpi=80)\n",
    "\n",
    "# plot x and y\n",
    "plt.scatter(x,y)\n",
    "\n",
    "# add labels to the dots so we know what each of them refers to\n",
    "for i, txt in enumerate(tokens_to_plot):\n",
    "    plt.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "# add a grid to help understand distances a bit better\n",
    "plt.grid()\n",
    "\n",
    "#show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this visualisation may not make a lot of sense.\n",
    "\n",
    "However, here we're just plotting the embeddings of a very very very small model. So small that there was not enough data to find relationships between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise:**\n",
    "\n",
    "Load a different model (a real model, not a test model) and plot some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise:**\n",
    "\n",
    "Can you think how this could be used as a way to perform 'bias detection'? Experiment with a model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lE0rSC2VzA0r",
    "HEFmGiSTzA0t"
   ],
   "name": "word2vec_dhoxss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "54ed4be044bdbfddf339b8bedcd1bd34fa47dd0b44ee203d7e74423349b7c92b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
