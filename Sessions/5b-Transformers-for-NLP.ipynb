{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854f0527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recent advances in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b486995",
   "metadata": {
    "id": "cyF-X7Ho0E--",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Static word embeddings\n",
    "\n",
    "Introduced in 2013, word2vec has had a huge impact in natural language processing and its applications.\n",
    "\n",
    "Vector representations of words seem to capture word meaning quite well!\n",
    "\n",
    "Accessible and easy to use (easy to train, to apply and to share).\n",
    "\n",
    "Shortcoming: this algorithm creates static embeddings, i.e. it creates one vector per word, no matter how many meanings the word has (e.g. `I like apples` vs `I like Apple macbooks`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d04f22",
   "metadata": {
    "id": "1yVysUSK0yI8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Import the `gensim` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a902344",
   "metadata": {
    "id": "IcAc17ZA01yc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3435fd",
   "metadata": {
    "id": "eiiasj951mOB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Download and load one of the models.\n",
    "\n",
    "Just for illustration, we'll use `glove-wiki-gigaword-50`, which was trained on text from Wikipedia and Gigaword (newswire). Note that different models may perform differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c145005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVR6arbK15D7",
    "outputId": "4e138515-7f72-4c4b-b789-9a12908b0d2d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa001e50",
   "metadata": {
    "id": "feiy-OWs3Au-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Static word embeddings create one vector per word.\n",
    "\n",
    "_Example 1:_\n",
    "See top 20 most similar words to word 'mouse'. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd0c8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1CWM5FQ2NFl",
    "outputId": "123fbf85-c03b-4733-8ea0-6ae4bdd28d70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('mouse', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1038d4",
   "metadata": {
    "id": "xWzLG0mC3d9J",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Example 2:_ See top 20 most similar words to word 'pear'.\n",
    "\n",
    "What would you expect to see here? Any guesses on the top most similar? And what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689031c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kft1MBsW2wRO",
    "outputId": "42759d8b-d8a3-4e51-ba92-3f19b57b7f8b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('pear', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff12fa",
   "metadata": {
    "id": "xWzLG0mC3d9J",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Example 3:_ See top 20 most similar words to word 'apple'.\n",
    "\n",
    "What would you expect to see here? Any guesses on the top most similar? And what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707fed8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zik2L8-X2z0y",
    "outputId": "26b82866-7950-4b19-e204-4cda9c260cc1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('apple', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f49c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word2Vec (2013) was one of the most important developments in NLP:\n",
    "* It captures word meaning quite well!\n",
    "* Very accessible and easy to use\n",
    "\n",
    "... but it produces \"static embeddings\", with one vector representation for each word form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fbe2ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**But most words have multiple meanings!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8c22a",
   "metadata": {
    "id": "FzvqY_p20jj9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Contextualized word embeddings\n",
    "\n",
    "Words mean different things in different contexts.\n",
    "\n",
    "**Goal:** learn the representation (i.e. the \"meaning\"!) for each word in its context.\n",
    "\n",
    "In recent years (since 2018 mostly), lots of progress has been made (from BERT to GPT-3).\n",
    "\n",
    "Also, lots of progress in making this easily accessible, and easy to use. The company HuggingFace has been greatly responsible for this last point, especially with their `transformers` library and their model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2779d65",
   "metadata": {
    "id": "fdAWG8oKZ6AW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **transformer** is a deep learning model that uses the **attention** mechanism (a mechanism which is based on cognitive attention, and which focuses on where the key information in a sequence is produces while forgetting less relevant information). Its development has had a huge impact in deep learning, especially in natural language processing and computer vision. It allows a more effective modeling of long term dependencies between the words in a sequence, and more efficient training, not limited by the sequence order of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f67a39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**BERT** (Bidirectional Encoder Representations from Transformers) is a transformer-based model, hugely successful, that creates contextualized word embeddings, it captures fine-grained contextual properties of words. It learns contextualized information through a masking process (i.e. it hides some words and uses their position to infer them back)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0ed99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/mask_predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72be8b2",
   "metadata": {},
   "source": [
    "## 3. HuggingFace 🤗\n",
    "\n",
    "HuggingFace is a company specialised in developing NLP technologies. \n",
    "\n",
    "Their open source `transformers` library has become one of the most popular libraries for NLP:\n",
    "* State-of-the-art NLP easier to use.\n",
    "* Provides APIs to download and use pretrained models, but also allows you to load and fine-tune your own models.\n",
    "* It is open source! \n",
    "* Maintains a **model hub**: central point for people to share and find models. They host more than 50K models, supporting different languages and different tasks, and also more than 7K datasets.\n",
    "\n",
    "We'll just scratch the surface, but if you are interested in this, we highly recommend the HuggingFace course: https://huggingface.co/course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2386907",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The `transformers` pipelines\n",
    "\n",
    "The `transformers` library provides an easy way of using transformer models for some of the main tasks in natural language processing, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c63130",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sentiment analysis\n",
    "\n",
    "The task of determining whether a text is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98109c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/sentiment_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7cbb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zero-shot classification\n",
    "\n",
    "The task of classifying texts into categories of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06a73b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/zeroshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf0669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text generation\n",
    "\n",
    "The task of generating text given a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63a72f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/textgen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa85f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Named entity recognition\n",
    "\n",
    "The task of recognizing named entities in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f9b25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/ner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcfdc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Machine translation\n",
    "\n",
    "The task of converting a source text from one language to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cf01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/machinetransl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73e2b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question answering\n",
    "\n",
    "The task of retrieving the answer to a question from a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e454f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/questionans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b357e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercises:**\n",
    "    \n",
    "* Explore the model hub: https://huggingface.co/models.\n",
    "* Have a look at the models: tasks and languages available.\n",
    "* Discussion:\n",
    "    * Can you find a model that fits your needs (language, task, ...)?\n",
    "    * Have you spotted any strange behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bc687",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to use `transformers` in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13b9d9",
   "metadata": {
    "id": "mnbqG479I1ag",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will first install the `transformers` library (and dependencies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eade367",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa35b5",
   "metadata": {
    "id": "ZV5IsTTgZlVX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Import the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94227fbb",
   "metadata": {
    "id": "P6wepbWPZjQy",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1c8ff",
   "metadata": {
    "id": "leCcO-ZoRchV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Using BERT pipelines\n",
    "\n",
    "Pipelines are a simplified way to apply BERT models (and other transformer models!). A pipeline is a code object that abstracts most of the complex code (it happens in the background), leaving only the bare minimum for the user to interact.\n",
    "\n",
    "We load the `pipeline` module from the `transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49370e6",
   "metadata": {
    "id": "vs1_41BRPKgD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046d7fb",
   "metadata": {
    "id": "38gnrw2gNVbs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To create a pipeline, you need to know:\n",
    "* Which task you want to perform (e.g. `'fill-mask'`)\n",
    "* The model you want to use to make predictions (e.g. `'distilbert-base-uncased'`), which must be trained for the task you want to perform (i.e. `fill-mask`).\n",
    "* The tokenizer used by the model (i.e. the strategy that BERT uses to split sequences into smaller units. This is often the same name as the model, e.g. `'distilbert-base-uncased'`).\n",
    "* Which conventions the language model follows: e.g. if your task is `fill-mask`, how is the masked element tagged (usually `[MASK]`, sometimes `<MASK>`, etc.).\n",
    "\n",
    "If you obtained your model from the HuggingFace model hub (https://huggingface.co/models), you should be able to find all this info in the model card (e.g. https://huggingface.co/bert-base-uncased).\n",
    "\n",
    "**Note:** You can find more information on pipelines and how to use them in https://huggingface.co/transformers/main_classes/pipelines.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3f2bd",
   "metadata": {
    "id": "wHftD07XWP8j",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. The Mask filling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b7cd08",
   "metadata": {
    "id": "Fw-gYTxGToyX",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token (source: https://huggingface.co/transformers/task_summary.html#masked-language-modeling). The `fill-mask` pipeline replaces the mask in a sequence by the most likely prediction according to a BERT model.\n",
    "\n",
    "We will create a `fill-mask` pipeline using the `distilbert-base-uncased` English model (and its tokenizer), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01e562",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "referenced_widgets": [
      "c4127d0a8863429baca65151680237fb",
      "07ed00183908469f89f0714c94b8be66",
      "892489a1a22d452eb2b8017231a49f63",
      "fe6c962147dd4250b8099cca88c4e82b",
      "ee0fd2e49c694c5cace010298133b86e",
      "5d3715e3b70c4b3aa95a95d157f7ba3a",
      "f0c092ac58e44e1fb62f51eb301339ef",
      "73ae6c09260a4101af78e91a2f67ee2f",
      "248b91fa193f43cd912431e6034116cd",
      "9ab77b5d3cad496a994d549fd84fcabe",
      "24487affaded4b24905b4398280cc16c",
      "ccfafaf56b1443d99b50308a5dd6c16d",
      "47ba9e6233e4448faa9435907c8a5589",
      "27ee1fc1ea4f49c2b769d3c0cb92fbcd",
      "605619be5e514845a5b5ab2a0319cb23",
      "96f7c81bce7f43449e1c2e17c083be1b",
      "537fe636ee6f40afa33d04a2e340311b",
      "a45ec38265ec4995927d545c9b12eddd",
      "90308ebc135a4777b26b75ce2a9948a8",
      "8460c6841339421580488018abff4d92",
      "cca00ac611bd4765a5d78d8001d0a4c3",
      "bda05c808a084aea9521d1925895890d",
      "88f64238a98e4f8f9bfb4c39abad112b",
      "fc631c664369444f96b141e235ed9a01",
      "8ba8bc7234534f84b4be792131473e2e",
      "7b90bd58f453439a83ce9ae244f3c844",
      "0e3665460a7c4f27a31fb1f2a026e52e",
      "878e15bc14e24bfd969dfa0bafe57a46",
      "34b9668e352546e7a48e2f7ef01b6ece",
      "58dfdaf7f38d4c60926b958ed8702329",
      "fa36fe68f2504d22a022302277f74e83",
      "37948521405c4f9eadb7ad71ca677474",
      "1ad2a42bc785434aa0a1186f42345f37",
      "1b0bece128bb4490aac5da3fc465f08f",
      "47bc5ae05b0a4458ba37f4f7fa3d933d",
      "f69252a6efd34300825ecf215e4b5db0",
      "f24dd8ac049c48d397032e228134f82d",
      "42f55e703d9d45a5abc6dd3883b3e0c3",
      "418a65725a6a4ccf9df83c086b84d1a7",
      "778718ab1d1540468af783745300ecc6"
     ]
    },
    "id": "QExSFUJuPKiX",
    "outputId": "20bddc4d-9db7-4d4a-ceec-13b53719ef04",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask',\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba632d1",
   "metadata": {
    "id": "75GM6hUbUBXc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This pipeline allows us to easily use BERT to predict the masked element in a sentence.\n",
    "\n",
    "In the previous cell, we are:\n",
    "* Creating a pipeline for the task of `fill-mask`,\n",
    "* by using the `distilbert-base-uncased` BERT model and tokenizer,\n",
    "* and storing the resulting pipeline in a variable (we call it `unmasker`), which we can use and reuse in subsequent code.\n",
    "\n",
    "**Warning:** You need to make sure the model you use is trained for the `'fill-mask'` task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046d623",
   "metadata": {
    "id": "Kg4Ta73vSFY3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use the pipeline, you just need to pass the sentence containing the masked word as an argument of `unmasker` (i.e. the variable containing your pipeline). You don't need to do any encoding, the pipeline already takes care of converting the text into an input BERT can understand!\n",
    "\n",
    "We store the output of applying the pipeline to this sentence in the `outputs` variable, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a554f1a",
   "metadata": {
    "id": "AEBgbIZSR_UH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"The cell is guarded by a [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac93c3",
   "metadata": {
    "id": "3BFrn3ZqSVg4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's inspect the `outputs` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3f9ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQYwNfXQSoHg",
    "outputId": "bedb5c97-863d-483b-a7b5-f0c9ac96bd3a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4c4b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxBd5fa3TXhm",
    "outputId": "667ccdac-26f6-4dbe-8ad4-2bbcaf8f6379",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79563a74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5UZFsqzcZvt",
    "outputId": "c01331e0-fca6-43b8-eaa9-985f0d60d2d5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"\"\"When a cell has been produced, we can then trace some of the\n",
    "                      stages by which new [MASK] are formed. There appear to be four\n",
    "                      modes in which vegetable cells are multiplied. The new cells\n",
    "                      may either proceed from a nucleus or they may be formed at\n",
    "                      once in the protoplasm.\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4d510",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2Ia8CvZdEXV",
    "outputId": "0e8c960f-d059-4e2c-9a9b-d2860553a654",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"\"\"Imprisonment with proper employment, and at least two visits\n",
    "                      every day from a prison officer. The punishment does not\n",
    "                      extend over a month. A week must elapse before the same\n",
    "                      prisoner can be put again into the dark [MASK].\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc6d78",
   "metadata": {
    "id": "uSaj5vRG1cx-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578c7ff",
   "metadata": {
    "id": "gnE7bkL3sAoB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Find a `fill-mask` model from HuggingFace model hub (trained on data in your preferred\n",
    "# language, if there is one). Create a `fill-mask` pipeline and try to predict the mask\n",
    "# token in some sentences.\n",
    "# * Try this with different sentences.\n",
    "# * What do the scores indicate?\n",
    "# * Try to see what happens if you want to use BERT to predict something that requires\n",
    "#   world knowledge, for example:\n",
    "#     * `Everyone agrees that the princes in the tower were [MASK].`\n",
    "#     * `It would seem [MASK] III killed the princes in the tower.`\n",
    "#     * `Barcelona is a city in [MASK].`\n",
    "#     * `Paris is the capital of [MASK].`\n",
    "#\n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417043",
   "metadata": {
    "id": "t--wEQzKvfls",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Load and use your own models\n",
    "\n",
    "In this tutorial we won't have time to cover how to train or fine-tune your own BERT model, but at the end of this notebook you will find some links on this.\n",
    "\n",
    "We will now imagine you have your own BERT models you want to use. Instead, we will be using our historical English BERT models, just to show that you can also use the `transformers` library using your own model. You just need to correctly point the right path to the model when loading it.\n",
    "\n",
    "See how we load our historical English BERT models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ff638",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **To do:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b233e34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zq6k6lFtnGfE",
    "outputId": "5ded51dd-2cd7-4966-de4c-4f36e76bf67c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have stored our historical English BERT models stored in [Google drive](https://drive.google.com/drive/folders/1Y-ltpJNCfTO0ti7zPnBdRWlyMXh8OjmH?usp=sharing).\n",
    "These language models are described in [this paper](https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.48/).\n",
    "\n",
    "!!! Important facts you will **need to know** about these language models:\n",
    "* They were fine-tuned on the `fill-mask` task based on `bert-base-uncased`\n",
    "* They use the `bert-base-uncased` tokenizer.\n",
    "\n",
    "The dataset on which these language models are trained is a 19th-century collection\n",
    "of books in English. We will download the following two BERT models:\n",
    "* `bert_1760_1850.zip`: trained on books from 1760 to 1850: [download](https://drive.google.com/file/d/1QJgUFiFgplOq2eBUn5mLwAxcn3KOSPxw/view?usp=sharing)\n",
    "* `bert_1890_1900.zip`: trained on books from 1890 to 1900: [download](https://drive.google.com/file/d/1nPlcyBBOdGYxRGVmiCrgC6muhgD87lva/view?usp=sharing)\n",
    "\n",
    "Download the files, unzip them, and store the `bert_1760_1850` and `bert_1890_1900` folders\n",
    "directly under the `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3306c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "d0b22839bf2e443995e167b46eac6069",
      "9098c0ae1fc4423d92964a6e50a013f6",
      "d287a9759eb849e3823f4ebf42db3b53",
      "144958ab732c4cdfa23b9034aebc6efc",
      "a5faa6fb33e542669d3877f03442292a",
      "5f6d7a5cc9c04db1b65202e4314b8323",
      "d527ecb6504149e69b9503d29d194a02",
      "a13dbcf9ae50468dae5ef14dd80872f9",
      "9efe21c6002640edb3ebd9805de32798",
      "3eae2c6fd4f8467eb6e2279873b2ea1a",
      "81a8108e5c4c4590b1955ce64edc8ac1",
      "3562bc738d9043ba92501eff95f2bbd8",
      "e369ef22237e41989bd8ff0f618146f6",
      "4c96e2ab39c74ff6be2e1c00f5a5041b",
      "aad72ac1a14349d7af0593ded98deafd",
      "9180f9f83db2476e8c66d2d85880fc60",
      "ea0ba5ec533c4f1c85a93636c3d3838c",
      "f505a4942dd24b9599c0aaeca9f6e738",
      "55f1137cdfc84aa380a761bd4dd6f95d",
      "34c6e2eca6a84b7a966f60ae732cbcaa",
      "fb420fdbbc0748e7bfb067c7fd79c81e",
      "7cf0730cea6c44dea0a9526bb3c9abd9",
      "e0c85de17cbb41d1b33995ecd90c46fe",
      "b8ce9ad0303143568f87542d90226a46",
      "cf8a3148b7d04b90955d3da4edb69adb",
      "ff0fad8ad81847a4ab4529b2f7ab372e",
      "57244ef4ea4140efa2ac5587452f3bda",
      "15517a901d8b43b399034a97756388a8",
      "4fd675ee3e4b4260b0a0c64f5994a9be",
      "1be9319d79fd48559533e8388c763a5a",
      "e4901e64d8a444898f5dc08c338189fb",
      "4055e61b146e4e6f990f7003228710a6"
     ]
    },
    "id": "LckRxq-V1ATz",
    "outputId": "3cb5aa1e-8646-48ac-cd0f-7cebe91fa420",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can create a `fill-mask` pipeline for the 1760-1850 model. To do so, you\n",
    "# just need to add the path to the `model` argument. It is very important that\n",
    "# you know (1) which is the tokenizer that was used to train the model and (2)\n",
    "# on which task the model was fine-tuned, in this case `fill-mask`: this info\n",
    "# is usually given in the description of the model.\n",
    "unmasker_1760_1850 = pipeline('fill-mask',\n",
    "                              model='models/bert_1760_1850',\n",
    "                              tokenizer='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2929b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_rzisZfnGkN",
    "outputId": "9397232a-25a6-4689-f854-80da7499ce0e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can now use them to predict a mask in a sentence as well:\n",
    "outputs = unmasker_1760_1850(\"\"\"The [MASK] is guarded by guards.\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008dc427",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f805d6",
   "metadata": {
    "id": "1s9ORTYrPKkB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for the 1890-1900 model as well and try different sentences with\n",
    "# both the 1760-1850 and the 1890-1900 models. Do language models trained on data\n",
    "# from different periods make different predictions?\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03574377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. The other pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40e72f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These are other pipelines available through HuggingFace:\n",
    "* `ner` (for named entity recognition)\n",
    "* `question-answering`\n",
    "* `sentiment-analysis`\n",
    "* `summarization`\n",
    "* `text-generation`\n",
    "* `translation`\n",
    "* `zero-shot-classification`\n",
    "\n",
    "HuggingFace have created well-documented [tutorial](https://huggingface.co/course/chapter1/3?fw=pt#working-with-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0979e30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d6bb7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Have a look at the other pipelines, get inspration from the tutorial:\n",
    "# https://huggingface.co/course/chapter1/3?fw=pt#working-with-pipelines\n",
    "# Play a bit with different pipelines, using different models from the\n",
    "# model hub: https://huggingface.co/models (make sure that the language\n",
    "# is trained for the task you would like to try!).\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47d743",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.5. Get the vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d9998",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The meaning of a word, in NLP, is usually represented as vectors, i.e. lists of numbers. In the case of transformer models (such as BERT), the vector of a word changes based on the context in which this word occurs. In the following cells, we'll see how to obtain the vector representation of a token using the `tranformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7a755",
   "metadata": {
    "id": "PCx9f4pzZpTo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenization\n",
    "\n",
    "As we saw yesterday, tokenizing a text is splitting it into meaningful units. Very often, this means splitting a text into words.\n",
    "\n",
    "BERT uses a **subword** tokenization procedure called WordPiece.\n",
    "\n",
    "This means that it does not only separate words. It also splits certain words into (ideally) meaningful units\n",
    "> E.g. it splits the word `tokenizing` into `token` and `##izing`, where `##` indicates that this is a suffix which should be attached to the previous word).\n",
    "\n",
    "These tokens are then mapped to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae03938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tokenizer maps every word form (e.g. `token` and `##izing`) with identifiers in the vocabulary (e.g. given a certain model, `19204` is the vocabulary ID of `token` and `6026` is the vocabulary ID of the suffix `##izing`, so the word 'tokenizing' would be tokenized as `[19204, 6026]`).\n",
    "\n",
    "**!!! Warning:** BERT has certain limits as to the length of the string that is accepted, which depends on the model, but usually 512 tokens.\n",
    "\n",
    "**!!! VERY important:** different models may have different token-to-id mappings. When we use an existing model, we must use the same tokenizer (and therefore the same vocabulary mapping) that was used when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445a44f",
   "metadata": {
    "id": "-P5gKh2GqWS4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The inner workings of BERT tokenization\n",
    "\n",
    "Tokenization steps:\n",
    "\n",
    "1. The text is split into tokens, which can be:\n",
    "  * words\n",
    "  * parts of words\n",
    "  * punctuation symbols\n",
    "\n",
    "2. The tokenizer adds special tokens:\n",
    "  * `[CLS]` indicating that this is the beginning of the input sequence.\n",
    "  * `[SEP]` indicating that it is the end of the sequence (or a sequence delimiter if we have a pair of sequences as input).\n",
    "\n",
    "3. The tokenizer maps each token into their vocabulary IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6080ce5",
   "metadata": {
    "id": "mQFYLklrpQRz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ebb40",
   "metadata": {
    "id": "RthE6sQiZXqh",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer of a certain BERT model\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cf6ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRczZfPRZXs3",
    "outputId": "8590811c-1611-4e7e-8777-7b73899ebf99",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The `encode` function does the three steps in one go (splits the sequence, adds\n",
    "# special tokens, and converts them into a sequence of IDs):\n",
    "encoded_seq = our_tokenizer.encode('The cell is relentlessly guarded by guards.')\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27946189",
   "metadata": {
    "id": "_E_CFHlvNIfD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And there are also functions that translate the vocabulary IDs to the word forms (given a certain tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923b0b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRuw2R-2ZXxT",
    "outputId": "144a8bd2-add2-46e3-a96c-8964e7b22266",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The `convert_ids_to_tokens` returns the tokens that correspond to the IDs of\n",
    "# an encoded sequence:\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989aadf",
   "metadata": {
    "id": "JzUAk_78av8c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.6. The feature extraction pipeline\n",
    "\n",
    "Here we will see how to get vectors for words in context.\n",
    "\n",
    "Similarly to what we did with word2vec, we may also want to have access to the vector of a certain word. However, unlike with word2vec, the vector of a word will depend on the context in which the word occurs. This means that we can't just ask for the vector of the word \"apple\", for example: we will need to ask for the vector of the word \"apple\" given a certain context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151f976",
   "metadata": {
    "id": "Oh1tR1gk7FiO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We first import the following two libraries, which will help us work with vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db625909",
   "metadata": {
    "id": "0qGhhLcKbLct",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # python library used for working with vectors\n",
    "from scipy import spatial # package to help compute distance or similarity between vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda651",
   "metadata": {
    "id": "yaiUaFkM8Epq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pipeline task to obtain the vectors for tokens in a sequence is `feature-extraction`. As you can see, creating this pipeline is very similar to creating the `fill-mask` pipeline.\n",
    "\n",
    "We will store the pipeline in a variable called `nlp_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112889d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-XAVDugazIe",
    "outputId": "2f246551-6185-4fa8-b99b-08af2847cbdd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp_features = pipeline(\"feature-extraction\",\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad0158",
   "metadata": {
    "id": "6Lysu1Dm_Weh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given a sentence, the pipeline tokenizes the input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d1b24",
   "metadata": {
    "id": "xZV_uhZ2-G1V",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"They were told that the machines stopped working.\"\n",
    "\n",
    "output = nlp_features(sentence)\n",
    "output_vectors = np.squeeze(output) # This removes single-dimensional entries (i.e. for vector readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07db6c9",
   "metadata": {
    "id": "B11uYzovCe8z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's inspect the output. First of all, let's print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef4581",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkcTRDTeCoN0",
    "outputId": "c9a2dfbe-2f49-4718-d9b2-ed877e154a60",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44507",
   "metadata": {
    "id": "3SCm6NODCjon",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is an array (a list of vectors). Let's see its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b4ace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xNx4rwN_Qkk",
    "outputId": "0b95c6d9-055b-4ca5-ed80-1d09def551b3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(output_vectors.shape) # Print the shape of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff11aa",
   "metadata": {
    "id": "pkrr6zpPEGI5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that we have an arrray (in other words a matrix, a table) that has 11 vectors of length 768 (or, in other words, 11 rows with 768 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7a1ea",
   "metadata": {
    "id": "FIrLMeHlG0gG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question:** 11 vectors? Why 11?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743ae39",
   "metadata": {
    "id": "uuGWf0R_LV8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how the sentence is tokenized (we've seen how above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94808203",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeDbMIQFLY-E",
    "outputId": "e2f5d37d-3e2a-418a-b0fc-5b224d64e275",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the **SAME** tokenizer used in the pipeline:\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Encode the sentence into a sequence of vocabulary IDs\n",
    "encoded_seq = our_tokenizer.encode(sentence)\n",
    "print(encoded_seq)\n",
    "\n",
    "# And get the tokens given the vocabulary IDs\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)\n",
    "\n",
    "# And print the length of the tokenized sequence:\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a177281",
   "metadata": {
    "id": "IrgIvkB0G35O",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, the input sentence has been tokenized into 11 tokens. So what we have in the above array is 11 vectors (each one representing a word in the context of the sentence, **keeping the order of tokens**, i.e. the first vector will correspond to the special token `[CLS]`, the second vector to the token `the`, and so on until the last vector, which corresponds to the special token `[SEP]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dec8e",
   "metadata": {
    "id": "g60E3KzsOGzT",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we get the vector of a specific token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92a97b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0ATT1dKOZNw",
    "outputId": "8dbe0fc3-eeb3-43e0-9c2c-44e70a1103cb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tokens[6]) # The 6th element in the tokenized sentence is the token `machine` (we start counting from zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe1264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNMhJtHgIkFj",
    "outputId": "6484e968-a6aa-470a-fb0c-e64005baf543",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(output_vectors[6]) # Therefore, o the 6th vector in output_vectors is the vector of `machine` in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fe5bb",
   "metadata": {
    "id": "sYgcioF-_q9q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67909039",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create two `feature-extraction` pipelines, one for the  1760-1850 model, and\n",
    "# one for the 1890-1900 model. Find whether the cosine similarity between words\n",
    "# in sequences change depending on which BERT model you use.\n",
    "# \n",
    "# Type your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32e801",
   "metadata": {
    "id": "RLnsKVzkX-EO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "👀 **If you are interested in knowing more, we recommend:**\n",
    "* The HuggingFace tutorials: https://huggingface.co/course/chapter1/1\n",
    "* BERT for Humanists: http://www.bertforhumanists.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8f26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
