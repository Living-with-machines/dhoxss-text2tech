{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50f7f68",
   "metadata": {},
   "source": [
    "# Pocking at Ever Larger Language Models\n",
    "## An introduction to language models for (digital) humanists\n",
    "### From Neural to Pretrained Language Models\n",
    "\n",
    "\n",
    "Sources used in this tutorial\n",
    "- The Programming Historian lesson on [Interrogating a National Narrative with GPT-2\n",
    "](https://programminghistorian.org/en/lessons/interrogating-national-narrative-gpt).\n",
    "- Jurafsky & Martin, Speech and Language Processing (3rd ed. draft), available [online](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24d63",
   "metadata": {},
   "source": [
    "## What are language models?\n",
    "\n",
    "LMs tell us what is likely to come next in a sequence. More technically:\n",
    "\n",
    "> “[Language models] assign a probability* to each possible next word. (Jurafsky & Martin)”\n",
    "\n",
    "Given the sentence **“Predicting the future is hard, but not …”**\n",
    "\n",
    "- P(“impossible” | sentence) is greater than P(“aardvark” | sentence)\n",
    "\n",
    "\n",
    "> Read P(“impossible” | sentence) as the probability of observing the token “impossible” given the sequence “Predicting the future is hard, but not ...\n",
    "\n",
    "\n",
    "> Probabilities are values between 0 and 1 that sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f024e05",
   "metadata": {},
   "source": [
    "**Peaking ahead**: if you can predict what comes next in a text sequence you learn quite a lot about language use and the world in general.\n",
    "\n",
    "- Paris is located in [BLANK]\n",
    "- He was late. I was really angry and told [BLANK]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec36b9a",
   "metadata": {},
   "source": [
    "## Quick recap\n",
    "- Language modelling is the task of predicting the next word *w* given a history *h* (i.e. **P(w | h)**).\n",
    "- At each step, we can compute the probability over all the following words.\n",
    "- We can measure the **performance** of a model by evaluating how well a model can predict the next word (it will assign higher probabilities to actual texts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077586e",
   "metadata": {},
   "source": [
    "## Pretrained Language Models\n",
    "\n",
    "- Transition from N-Gram to Neural Language Models (ca. 2013)\n",
    "    - Word2Vec: Predict the **center word** given a context of n words, or predict context given a center word (**fixed windows**)\n",
    "- Around 2017-18 Pretrained Language Models took over\n",
    "    - **PLMs** trained with a different objective: predict the next word given a sequence (or predict masked words in a sequence (**variable length**))\n",
    "    - Models become 'larger', **more parameters**, and can model word meaning in context (instead of having a global meaning representation). To be explained in more detail later!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f8a5b",
   "metadata": {},
   "source": [
    "## Terminology: Parameters and size, what are we talking about?\n",
    "\n",
    "<img src=\"https://soundgas.com/wp-content/uploads/2021/02/Vintage-mixers-from-Roland-Yamaha-1024x576.jpg\" alt=\"knobs\" width=\"500\">\n",
    "\n",
    "- Parameters are \"knobs\" you can adjust to transform an input to the output you want\n",
    "- For a language model, the input is a sentence, the output is a probability over words (which should resemble the actual next word)\n",
    "- Deep Learning algorithms attempt to find the optimal setting of these knobs. The more knobs, the more complex stuff you can do (but equally, it becomes harder to understand how the machine actually works).\n",
    "\n",
    "![simpleNN](https://miro.medium.com/v2/resize:fit:624/1*U3FfvaDbIjr7VobJj89fCQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9358af",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Common PLM variants\n",
    "\n",
    "\n",
    "Determined by the language modelling task:\n",
    "- **Causal/Autoregressive Language Models** (GPT series): Predict the next [BLANK]\n",
    "- **Masked Language Models** (BERT and family): Predict the [BLANK] word.\n",
    "\n",
    "Let's have a closer look at a real language model: GPT-2. To work with (large) language models, we will be relying on the HuggingFace's `transformer` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d127e3",
   "metadata": {},
   "source": [
    "## Hugging Face 🤗 and the Transformers library\n",
    "\n",
    "Hugging Face is a company specialised in distributing deep learning models and data. \n",
    "\n",
    "Their open source `transformers` library has become one of the most popular libraries for NLP:\n",
    "* State-of-the-art NLP easier to use.\n",
    "* Provides APIs to download and use pretrained models, but also allows you to load and fine-tune your own models.\n",
    "* It is open source! \n",
    "* Maintains a **hub**: central point for people to share (and find) **models** and **data**. They host more than 241K models, supporting different languages and different tasks, and also more than 45K datasets.\n",
    "\n",
    "We'll just scratch the surface, but if you are interested in this, we highly recommend the Hugging Face course: https://huggingface.co/course\n",
    "\n",
    "### What are Transformers (the T in GPT and BERT)\n",
    "\n",
    "A **transformer** is a deep learning model that uses the **attention** mechanism (a mechanism which is based on cognitive attention, and which focuses on where the key information in a sequence is produces while forgetting less relevant information). Its development has had a huge impact in deep learning, especially in natural language processing and computer vision. It allows a more effective modeling of long term dependencies between the words in a sequence, and more efficient training, not limited by the sequence order of the input sequence.\n",
    "\n",
    "You can read the original paper [here](https://arxiv.org/abs/1706.03762). It is by far the most impactful paper (in computer science and machine learning) of the last decade and 79588 citations on Google Scholar (last checked 29/06/2023 at 6:58 AM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb6cfe",
   "metadata": {},
   "source": [
    "### Install the required Hugging Face libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install transformers xformers accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7969",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2\n",
    "\n",
    "\n",
    "Why is generating texts interesting for DH research? \n",
    "- Can we use 'fictitious' data?\n",
    "- We are sampling from universe of texts that could have been...\n",
    "- ...but have experimental control by prompting the model in different ways?\n",
    "- If the model learns some valuable patterns and associations, we can interrogate it's behaviour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ffb1f",
   "metadata": {},
   "source": [
    "While more complex, GPT-2 operates similarly to a simple N-Gram LM.\n",
    "- Given a prompt or input sequence, it returns a **probability over the following word**\n",
    "- Then we can sample a word from this distribution, add it to the prompt, and repeat!\n",
    "\n",
    "Materials inspired by this [blog post](https://huggingface.co/blog/how-to-generate) and the excellent Programming Historian lesson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f52b23",
   "metadata": {},
   "source": [
    "## One token at a time: next word prediction with GPT-2\n",
    "\n",
    "Next word prediction is the building block of generative AI and we will also encounter it when playing with larger language models such as GPT-3 or ChatGPT.\n",
    "\n",
    "In the following example, we generate just one toke to show a language model creates a probability distribution over possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df902ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "import numpy as np\n",
    "from torch.nn import Softmax\n",
    "import pandas as pd\n",
    "# initialize softmax function\n",
    "softmax = Softmax(dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b942c",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Change the prompt and observe how the outcome distribution of words changes. We only look at the hundred most probably words.\n",
    "\n",
    "To code below is merely for illustrative purposes, don't worry if some things are unclear at the moment. If you want to delve deeper in machine learning with Hugging Face, there are excellent tutorials available. See for example the [Hugging Face Course](https://huggingface.co/learn/nlp-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello my name is' # define a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a text in units the LM is built on\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "# load the gpt-2 model\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "# get logits from model\n",
    "predictions = model(**tokenizer(prompt, return_tensors='pt')) \n",
    "# the predictions as logits\n",
    "# predictions.logits.shape \n",
    "# get words with highest probability\n",
    "tokenizer.decode(np.argmax(predictions.logits[0,-1,:].detach().numpy()))\n",
    "# order predictions\n",
    "series = pd.Series(softmax(predictions.logits[0,-1,:]).detach()).sort_values(ascending=False)\n",
    "# change token_ids to the actual tokens\n",
    "index = [tokenizer.decode(x) for x in series.index] \n",
    "# set tokens as index\n",
    "series.index = index \n",
    "# plot results\n",
    "series[:100].plot(kind='bar',figsize=(20,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b1441",
   "metadata": {},
   "source": [
    "## Generating texts from prompts\n",
    "\n",
    "The preceding process is rather cumbersone, we just generated one additional word. The `transformers` library provides more convenient functions for generating texts based on a prompt using the `pipeline` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', # define the task\n",
    "                     model = 'gpt2', # define the model\n",
    "                     pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa213409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = 'the duke of'\n",
    "#sequence = 'A no deal Brexit'\n",
    "prompt = 'The UK is' # select the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed577cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(prompt, \n",
    "            max_length = 30,  # max length of each generated text\n",
    "            num_return_sequences=3 # how many sequence to generate\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba79a3",
   "metadata": {},
   "source": [
    "### ✏️ 1. Exercise: \n",
    "\n",
    "Change the prompt and observe how this affects text generation. We only look at the hundred most probably words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf289390",
   "metadata": {},
   "source": [
    "## Refining Text Generation\n",
    "\n",
    "There are multiple settings we can adjust to drive the text generation in specific direction.\n",
    "\n",
    "## Temperature\n",
    "A very common parameter is `temperature` (which we will also encounter when playing with larger language models). \n",
    "\n",
    "Temperature regulates the creativity of a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbd00d",
   "metadata": {},
   "source": [
    "Increasing the temperature can make predictions more creative (or random if you [like](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.)))\n",
    "\n",
    "\n",
    "Image taken for this [blogpost](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71) on temperature in Softmax.\n",
    "\n",
    "![temperature](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          num_return_sequences=5,\n",
    "          do_sample=True, \n",
    "          top_k = 0,\n",
    "          temperature=.000000001, # change temparature to .7\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d0f2b",
   "metadata": {},
   "source": [
    "### Top k sampling\n",
    "\n",
    "To prevent that outliers will mess up the generation, you can restrict the options and select only a word from the k most probable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9c317",
   "metadata": {},
   "source": [
    "### Top p or nucleus sampling\n",
    "\n",
    "Another strategy is to sample from the smallest set of words whose cumulative probability exceeds the probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=0,\n",
    "          top_p=.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c668e",
   "metadata": {},
   "source": [
    "## Adapting a language model\n",
    "\n",
    "It is possible to change a language model by further training or fine-tuning it on new documents. Based on the tutorial on GPT-2 in Programming Historian we trained a model on news snippets related to Brexit. In other words, we've built a GPT-Brexit model on top of GPT-2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'Kaspar/gpt-brexit',tokenizer='gpt2',pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b594e",
   "metadata": {},
   "source": [
    "### ✏️ 2. Exercise: \n",
    "\n",
    "- Define a prompt implicitly related to Brexit (for example \"The UK is\").\n",
    "- Using the `pipeline` can you generate 3 documents with GPT-2 and GPT-Brexit.\n",
    "- Does this show interesting difference, how would you about studying these models? How valuable is text generation as a tool for DH research?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0877b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fd67d",
   "metadata": {},
   "source": [
    "### ✏️ 3. Exercise: \n",
    "\n",
    "Find another model for text generation on the Hugging Face hub, inspect the model card and generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e124d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98772e",
   "metadata": {},
   "source": [
    "## Modeling Meaning in Context with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8bbf",
   "metadata": {},
   "source": [
    "**BERT** (Bidirectional Encoder Representations from Transformers) is a transformer-based model that creates contextualized word embeddings: it is able to capture fine-grained contextual distinctions and properties of words. It learns contextualized information through a masking process (i.e. it hides some words and uses their position to infer them back)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ee823",
   "metadata": {},
   "source": [
    "Masked language modeling is the task of hiding tokens in a sequence with a **mask token**, and prompting the model to fill that mask with an appropriate token (source: https://huggingface.co/transformers/task_summary.html#masked-language-modeling). \n",
    "\n",
    "The `fill-mask` pipeline replaces the mask in a sequence by the most likely prediction according to a BERT model.\n",
    "\n",
    "We will create a `fill-mask` pipeline using the `distilbert-base-uncased` English model (and its tokenizer), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"When a cell has been produced, we can then trace some of the\n",
    "            stages by which new [MASK] are formed. There appear to be four\n",
    "            modes in which vegetable cells are multiplied. The new cells\n",
    "            may either proceed from a nucleus or they may be formed at\n",
    "            once in the protoplasm.\"\"\"\n",
    "\n",
    "outputs = masker(sentence)\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for o in outputs:\n",
    "    print(\"Prediction:\", o['token_str'])\n",
    "    print(\"Score:     \", round(o['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Imprisonment with proper employment, and at least two visits\n",
    "            every day from a prison officer. The punishment does not\n",
    "            extend over a month. A week must elapse before the same\n",
    "            prisoner can be put again into the dark [MASK].\"\"\"\n",
    "\n",
    "outputs = masker(sentence)\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for o in outputs:\n",
    "    print(\"Prediction:\", o['token_str'])\n",
    "    print(\"Score:     \", round(o['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7839ec",
   "metadata": {},
   "source": [
    "### ✏️ 4. Exercise: \n",
    "\n",
    "Think of another highly ambiguous word (e.g. \"bank\") and apply the same procedure as above to assess if BERT manages to distinguish the different senses. You can make this increasingly difficult, to test the limits of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7617ad4",
   "metadata": {},
   "source": [
    "### ✏️ 5. Exercise: \n",
    "\n",
    "Hugging Face provides BERT models in other languages, or even multilingual models. Search the hub for BERT (or similar masked language models) in any other language than English and apply the \"fill-mask\" pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad04ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830af87",
   "metadata": {},
   "source": [
    "### Tracing Semantic Change with Masked Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb740706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our sewing [MASK] stood near the wall where grated windows admitted sunshine, and their hymn to Labour was the only sound that broke the brooding silence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
    "print(masker(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "victorian_masker = pipeline(\"fill-mask\", model='Livingwithmachines/bert_1760_1850')\n",
    "print(victorian_masker(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b80b8",
   "metadata": {},
   "source": [
    "# Text Classification with BERT\n",
    "## The Living Machine case study\n",
    "\n",
    "Another popular task is supervised classification with text: \n",
    "- classify document according to particular categories\n",
    "\n",
    "This requires:\n",
    "- examples in the form of labeled data\n",
    "- a model that can learn the relation between the text and labels\n",
    "\n",
    "Once this is done (to some satisfaction) we can apply this model and \n",
    "    - genre classification\n",
    "    - emotion detection\n",
    "    - \"machine animacy\" or \"living machines\"\n",
    "    \n",
    "Again, the code below is more for illustrative purposes, and you are not required to understand everything. We merely want to show what steps are involved for training a text classification model. Much of this code will be useful if you'd like to create your own classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf51d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset, Value\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ab091",
   "metadata": {},
   "source": [
    "### Annotate and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1559d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from the huggingface hub\n",
    "dataset = load_dataset('biglam/atypical_animacy') \n",
    "# inspect the first three examples\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83fa7d",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10353d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns we don't need\n",
    "dataset = dataset.remove_columns(['id', 'context', 'target', 'humanness', 'offsets', 'date'])\n",
    "# rename column to a common format\n",
    "dataset = dataset.rename_columns({'animacy':'label','sentence':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially all data is training data\n",
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insepect the outcome of this process\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48926145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we want to do binary classification\n",
    "# we need to convert the labels from float to int \n",
    "# for this we can use the .cast() function in datasets\n",
    "new_features = dataset.features.copy()\n",
    "new_features[\"label\"] = Value(\"int32\")\n",
    "dataset = dataset.cast(new_features)\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82323dbf",
   "metadata": {},
   "source": [
    "### Split data into training and test set\n",
    "\n",
    "To train a model we need \n",
    "- **training set**: observations (examples and labels) used for finding the optimal parameters\n",
    "- **validation set**: observation used to monitor how the model performs during training\n",
    "- **test set**: a held-out set to evaluate the trained model (often not available). This tells us how well the model generalizes beyond the observations it has seen during during."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1643f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(dataset)*.3) # set 30% apart as test set\n",
    "train_test = dataset.train_test_split(test_size=test_size , seed=42) # split data into train and test\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.05) # use 5% of the training data for validation\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size,seed=42) # split into training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327264b",
   "metadata": {},
   "source": [
    "### Load a Pretrained Language Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased' # load a checkpoint \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) # load a tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2) # load a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147a59",
   "metadata": {},
   "source": [
    "### Tokenize and preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we proprocess data by converting texts to token ids\n",
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': 'sentence'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2dfb65",
   "metadata": {},
   "source": [
    "## Train Model on Annotated Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results\", # where to save the model\n",
    "    seed = 42, # ensure the experiment is reproducible\n",
    "    learning_rate=1e-3, # how aggressively should we update the model weights\n",
    "    per_device_train_batch_size=8, # how many examples per batch\n",
    "    per_device_eval_batch_size=8, # how many examples per batch\n",
    "    num_train_epochs=3, # how many times should we iterate over the training data\n",
    "    weight_decay=0.01, # how much should we reduce the reduce the learning rate\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # model to train\n",
    "    args=training_args, # training arguments\n",
    "    train_dataset=train_val[\"train\"], # training data\n",
    "    eval_dataset=train_val[\"test\"], # validation data\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=data_collator, # collates examples and presents tham as input\n",
    "        )\n",
    "\n",
    "trainer.train() # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0ba23",
   "metadata": {},
   "source": [
    "### Evaluate on test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.map(preprocess_function,fn) # preprocess test data\n",
    "predictions = trainer.predict(test_set) # get predictions, probabilities for each class\n",
    "preds = np.argmax(predictions.predictions, axis=-1) # get the most probable as the predicted class\n",
    "accuracy_score(preds,predictions.label_ids) # how accurate is the model, percentage correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543407e",
   "metadata": {},
   "source": [
    "## (Optional) The feature extraction pipeline\n",
    "\n",
    "Here we will see how to get vectors for words in context.\n",
    "\n",
    "Similarly to what we did with word2vec, we may also want to have access to the vector of a certain word. However, unlike with word2vec, the vector of a word will depend on the context in which the word occurs. This means that we can't just ask for the vector of the word \"apple\", for example: we will need to ask for the vector of the word \"apple\" given a certain context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1ed2f",
   "metadata": {},
   "source": [
    "We first import the following two libraries, which will help us work with vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3173bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # python library used for working with vectors\n",
    "from scipy import spatial # package to help compute distance or similarity between vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187e7db",
   "metadata": {},
   "source": [
    "The pipeline task to obtain the vectors for tokens in a sequence is `feature-extraction`. As you can see, creating this pipeline is very similar to creating the `fill-mask` pipeline.\n",
    "\n",
    "We will store the pipeline in a variable called `nlp_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead190c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_features = pipeline(\"feature-extraction\",\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50081723",
   "metadata": {},
   "source": [
    "Given a sentence, the pipeline tokenizes the input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"They were told that the machines stopped working.\"\n",
    "\n",
    "output = nlp_features(sentence)\n",
    "output_vectors = np.squeeze(output) # This removes single-dimensional entries (i.e. for vector readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e3bd0",
   "metadata": {},
   "source": [
    "Let's inspect the output. First of all, let's print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed0646",
   "metadata": {},
   "source": [
    "This is an array (a list of vectors). Let's see its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors.shape) # Print the shape of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bac26",
   "metadata": {},
   "source": [
    "This means that we have an arrray (in other words a matrix, a table) that has 11 vectors of length 768 (or, in other words, 11 rows with 768 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c09e73",
   "metadata": {},
   "source": [
    "**Question:** 11 vectors? Why 11?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0818f",
   "metadata": {},
   "source": [
    "Let's see how the sentence is tokenized (we've seen how above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the **SAME** tokenizer used in the pipeline:\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Encode the sentence into a sequence of vocabulary IDs\n",
    "encoded_seq = our_tokenizer.encode(sentence)\n",
    "print(encoded_seq)\n",
    "\n",
    "# And get the tokens given the vocabulary IDs\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)\n",
    "\n",
    "# And print the length of the tokenized sequence:\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89fde7",
   "metadata": {},
   "source": [
    "As you can see, the input sentence has been tokenized into 11 tokens. So what we have in the above array is 11 vectors (each one representing a word in the context of the sentence, **keeping the order of tokens**, i.e. the first vector will correspond to the special token `[CLS]`, the second vector to the token `the`, and so on until the last vector, which corresponds to the special token `[SEP]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82deecb9",
   "metadata": {},
   "source": [
    "How do we get the vector of a specific token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[6]) # The 6th element in the tokenized sentence is the token `machine` (we start counting from zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors[6]) # Therefore, o the 6th vector in output_vectors is the vector of `machine` in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5879f",
   "metadata": {},
   "source": [
    "### ✏️ 6. Exercise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ee168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two `feature-extraction` pipelines, one for the  1760-1850 model, and\n",
    "# one for the 1890-1900 model. Find whether the cosine similarity between words\n",
    "# in sequences change depending on which BERT model you use.\n",
    "# \n",
    "# Type your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a67ed",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
