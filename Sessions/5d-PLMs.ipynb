{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50f7f68",
   "metadata": {},
   "source": [
    "# Pocking at Ever Larger Language Models\n",
    "## An introduction for (digital) humanists\n",
    "### From Neural to Pretrained Language Models\n",
    "\n",
    "\n",
    "Sources used in this tutorial\n",
    "- programming historian\n",
    "- Jurafsky & Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24d63",
   "metadata": {},
   "source": [
    "## What are language models\n",
    "\n",
    "LMs tell us what is likely to come next in sequence. More technically:\n",
    "\n",
    "> ‚Äú[Language models] assign a probability* to each possible next word. (Jurafsky & Martin)‚Äù\n",
    "\n",
    "Given the sentence **‚ÄúPredicting the future is hard, but not ‚Ä¶‚Äù**\n",
    "\n",
    "- P(‚Äúimpossible‚Äù | sentence) is greater than P(‚Äúaardvark‚Äù | sentence)\n",
    "\n",
    "\n",
    "```Read P(‚Äúimpossible‚Äù | sentence) as the probability of observing the token ‚Äúimpossible‚Äù given the sequence ‚ÄúPredicting the future is hard, but not ...\"```\n",
    "\n",
    "\n",
    "```Probabilities are values between 0 and 1 that sum up to 1.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f024e05",
   "metadata": {},
   "source": [
    "**Peaking ahead**: if you can predict what comes next in a text sequence you learn quite a lot about language use and the world in general.\n",
    "\n",
    "- Paris is located in [BLANK]\n",
    "- He was late. I was really angry and told [BLANK]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec36b9a",
   "metadata": {},
   "source": [
    "## Quick recap\n",
    "- Language modelling is the task of predicting the next word *w* given a history *h* (i.e. P(w | h))\n",
    "- At each step, we can compute the probability over all the following words\n",
    "- We can measure the **performance** of a model by evaluating how well a model can predict the next word (it will assign higher probabilities to actual texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077586e",
   "metadata": {},
   "source": [
    "## Pretrained Language Models\n",
    "\n",
    "- Transition from N-Gram to Neural Language Models (ca. 2013)\n",
    "    - word2vec Predict the center word given a context of n words, or predict context given a center word (fixed context)\n",
    "    - PLMs: predict the next word given sequence or predict masked words in a sequence (variable length)\n",
    "    - Models become 'larger', more parameters. They can model token meaning in context\n",
    "\n",
    "## Terminology\n",
    "\n",
    "<img src=\"https://soundgas.com/wp-content/uploads/2021/02/Vintage-mixers-from-Roland-Yamaha-1024x576.jpg\" alt=\"knobs\" width=\"500\">\n",
    "\n",
    "- Parameters are \"knobs\" you can adjust to transform an input to the output you want\n",
    "- For a language model, the input is a sentence, the output is a probability over words (which should resemble the actual next word)\n",
    "- Deep Learning algorithms attempt to find the optimal setting of these knobs. The more knobs, the more complex stuff you can do (but equally, it becomes harder to understand how the machine actually works).\n",
    "\n",
    "![simpleNN](https://miro.medium.com/v2/resize:fit:624/1*U3FfvaDbIjr7VobJj89fCQ.png)\n",
    "\n",
    "\n",
    "\n",
    "## Common PLM variants\n",
    "- Causal/Autoregressive language models (GPT series): Predict the next [BLANK]\n",
    "- Masked Language Models (BERT and family): Predict the [BLANK] word.\n",
    "- By training a model on this task it learns a lot about language, and we can use this knowledge for generating new texts or other tasks.\n",
    "\n",
    "Let's have a closer look at a real language model, GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d127e3",
   "metadata": {},
   "source": [
    "## HuggingFace ü§ó and the Transformers library\n",
    "\n",
    "HuggingFace is a company specialised in distributing deep learning models and data. \n",
    "\n",
    "Their open source `transformers` library has become one of the most popular libraries for NLP:\n",
    "* State-of-the-art NLP easier to use.\n",
    "* Provides APIs to download and use pretrained models, but also allows you to load and fine-tune your own models.\n",
    "* It is open source! \n",
    "* Maintains a **model hub**: central point for people to share and find models. They host more than 50K models, supporting different languages and different tasks, and also more than 7K datasets.\n",
    "\n",
    "We'll just scratch the surface, but if you are interested in this, we highly recommend the HuggingFace course: https://huggingface.co/course\n",
    "\n",
    "### What are Transformers (the T in GPT and BERT)\n",
    "\n",
    "A **transformer** is a deep learning model that uses the **attention** mechanism (a mechanism which is based on cognitive attention, and which focuses on where the key information in a sequence is produces while forgetting less relevant information). Its development has had a huge impact in deep learning, especially in natural language processing and computer vision. It allows a more effective modeling of long term dependencies between the words in a sequence, and more efficient training, not limited by the sequence order of the input sequence.\n",
    "\n",
    "You can read the original paper [here](https://arxiv.org/abs/1706.03762). It is by far the most impactful paper (in computer science) of the last decade and 79588 citations on Google Scholar (last checked 29/06/2023 at 6:58 AM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb6cfe",
   "metadata": {},
   "source": [
    "### Install the required HuggingFacelibraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install transformers xformers accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7969",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2\n",
    "\n",
    "\n",
    "Why is generating texts interesting for DH research? Can we use fictitious data?\n",
    "- Sampling texts that could have been\n",
    "- If the model learns some valuable patterns and associations in a corpus, we can possibly by studying it's behaviour in reaction to prompts and new data\n",
    "    - a concrete example we will be looking\n",
    "        - GTP-Brexit\n",
    "        - Perception of Theresa May versus Boris Johnson\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ffb1f",
   "metadata": {},
   "source": [
    "While more complex, GPT-2 operates similarly to a simple N-Gram LM.\n",
    "- Given a prompt or input sequence, it returns a probability over the following word\n",
    "- Then we can sample a word from this distribution, add it to the prompt, and repeat!\n",
    "\n",
    "Materials inspired by this [blog post](https://huggingface.co/blog/how-to-generate) and the excellent Programming Historian lesson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f52b23",
   "metadata": {},
   "source": [
    "## Next word prediction with GPT-2\n",
    "\n",
    "Next word prediction is the building block of generative AI and we will also encounter it when playing with larger language models such as GPT-3 or ChatGPT.\n",
    "\n",
    "In the following example, we generate just one toke to show a language model creates a probability distribution over possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "import numpy as np\n",
    "from torch.nn import Softmax\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†tokenizer will split a text in units the LM is built on\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ee56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†load the gpt-2 model\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello my name is' #¬†define a prompt\n",
    "predictions = model(**tokenizer(prompt, return_tensors='pt')) #¬†get logits from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.logits.shape #¬†the predictions as logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†get words with highest probability\n",
    "tokenizer.decode(np.argmax(predictions.logits[0,-1,:].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax(dim=0) # initialize softmax function\n",
    "series = pd.Series(softmax(predictions.logits[0,-1,:]).detach()).sort_values(ascending=False)\n",
    "index = [tokenizer.decode(x) for x in series.index] #¬†change index to tokens\n",
    "series.index = index #¬†set tokens as index\n",
    "series[:100].plot(kind='bar',figsize=(20,5)) #¬†plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b1441",
   "metadata": {},
   "source": [
    "## Generating texts from prompts\n",
    "\n",
    "The preceding process is rather cumbersone, we just generated one additional word. The `transformers` library provides more convenient functions for generating texts based on a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa213409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = 'the duke of'\n",
    "#sequence = 'A no deal Brexit'\n",
    "sequence = 'The UK is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'gpt2',pad_token_id=tokenizer.eos_token_id)\n",
    "generator(sequence, max_length = 30, num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf289390",
   "metadata": {},
   "source": [
    "## Refining Text Generation\n",
    "\n",
    "There are multiple settings we can adjust to drive the text generation in specific direction.\n",
    "\n",
    "##¬†Temperature\n",
    "A very common parameter is `temperature` (which we will also encounter when playing with larger language models). \n",
    "\n",
    "Temperature regulates the creativity of a language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbd00d",
   "metadata": {},
   "source": [
    "Increasing the temperature can make predictions more creative (or random if you [like](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.)))\n",
    "\n",
    "\n",
    "Image taken for this [blogpost](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71) on temperature in Softmax.\n",
    "\n",
    "![temperature](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          num_return_sequences=5,\n",
    "          do_sample=True, \n",
    "          top_k = 0,\n",
    "          temperature=.000000001, #¬†change temparature to .7\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d0f2b",
   "metadata": {},
   "source": [
    "### Top k sampling\n",
    "\n",
    "To prevent that outliers will mess up the generation, you can restrict the options and select only a word from the k most probable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9c317",
   "metadata": {},
   "source": [
    "### Top p or nucleus sampling\n",
    "\n",
    "Another strategy is to sample from the smallest set of words whose cumulative probability exceeds the probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=0,\n",
    "          top_p=.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c668e",
   "metadata": {},
   "source": [
    "## Adapting a language model\n",
    "\n",
    "It is possible to change a language model by further training or fine-tuning it on new documents. Based on the tutorial on GPT-2 in Programming Historian we trained a model on news snippets related to Brexit. In other words, we've built a GPT-Brexit model on top of GPT-2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'Kaspar/gpt-brexit',tokenizer='gpt2',pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b594e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Define a prompt implicitly related to Brexit for example \"The UK is\"\n",
    "- Using the `pipeline` can you generate 3 documents with GPT-2 and GPT-Brexit\n",
    "- Does this show interesting difference, how would you about studying these models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0877b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fd67d",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Find another model for text generation on the Hugging Face hub, inspect the model card and generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e124d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98772e",
   "metadata": {},
   "source": [
    "## Modeling Word Meaning in Context with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8bbf",
   "metadata": {},
   "source": [
    "**BERT** (Bidirectional Encoder Representations from Transformers) is a transformer-based model, hugely successful, that creates contextualized word embeddings, it captures fine-grained contextual properties of words. It learns contextualized information through a masking process (i.e. it hides some words and uses their position to infer them back)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ee823",
   "metadata": {},
   "source": [
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token (source: https://huggingface.co/transformers/task_summary.html#masked-language-modeling). The `fill-mask` pipeline replaces the mask in a sequence by the most likely prediction according to a BERT model.\n",
    "\n",
    "We will create a `fill-mask` pipeline using the `distilbert-base-uncased` English model (and its tokenizer), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"When a cell has been produced, we can then trace some of the\n",
    "            stages by which new [MASK] are formed. There appear to be four\n",
    "            modes in which vegetable cells are multiplied. The new cells\n",
    "            may either proceed from a nucleus or they may be formed at\n",
    "            once in the protoplasm.\"\"\"\n",
    "\n",
    "outputs = masker(sentence)\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for o in outputs:\n",
    "    print(\"Prediction:\", o['token_str'])\n",
    "    print(\"Score:     \", round(o['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Imprisonment with proper employment, and at least two visits\n",
    "            every day from a prison officer. The punishment does not\n",
    "            extend over a month. A week must elapse before the same\n",
    "            prisoner can be put again into the dark [MASK].\"\"\"\n",
    "\n",
    "outputs = masker(sentence)\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for o in outputs:\n",
    "    print(\"Prediction:\", o['token_str'])\n",
    "    print(\"Score:     \", round(o['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7839ec",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Think of another highly ambiguous word (e.g. \"bank\") and apply the same procedure as above to assess if BERT manages to distinguish the different senses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7617ad4",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "HuggingFace provides BERT models in other languages, or even multilingual models. Search the hub for BERT (or similar masked language models) in any other language than English and apply the \"fill-mask\" pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad04ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830af87",
   "metadata": {},
   "source": [
    "### Tracing Semantic Change with Masked Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb740706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"Our sewing [MASK] stood near the wall where grated windows admitted sunshine, and their hymn to Labour was the only sound that broke the brooding silence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
    "print(masker(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "victorian_masker = pipeline(\"fill-mask\", model='Livingwithmachines/bert_1760_1850')\n",
    "print(victorian_masker(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b80b8",
   "metadata": {},
   "source": [
    "# Supervised Classification with BERT\n",
    "## The Living Machine case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf51d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset, Value\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ab091",
   "metadata": {},
   "source": [
    "### Annotate and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1559d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset atypical_animacy (/Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682308ea3a4d4cdea40e8370b886c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('biglam/atypical_animacy')\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83fa7d",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10353d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['id', 'context', 'target', 'humanness', 'offsets', 'date'])\n",
    "dataset = dataset.rename_columns({'animacy':'label','sentence':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b81cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a0863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'sentence', 'context', 'target', 'animacy', 'humanness', 'offsets', 'date'],\n",
       "    num_rows: 594\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48926145",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = dataset.features.copy()\n",
    "new_features[\"label\"] = Value(\"int32\")\n",
    "dataset = dataset.cast(new_features)\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82323dbf",
   "metadata": {},
   "source": [
    "### Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1643f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-9652ac799873fb0c.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-8482e57ede6cfa77.arrow\n",
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-70443c389b2cbb02.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-eab173811a52de51.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(dataset)*.3)\n",
    "train_test = dataset.train_test_split(test_size=test_size , seed=42)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.05)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327264b",
   "metadata": {},
   "source": [
    "### Load a Pretrained Language Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a182cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/d2/ydv0grbd38985h6_95t0vdjw0000gp/T/ipykernel_50393/1569015581.py\", line 3, in <cell line: 3>\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 467, in from_pretrained\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 2542, in from_pretrained\n",
      "    f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 433, in load_state_dict\n",
      "    state_dict = loader(os.path.join(folder, shard_file))\n",
      "NameError: name 'safe_open' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/kasparbeelen/anaconda3/envs/py39/lib/python3.9/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec147a59",
   "metadata": {},
   "source": [
    "### Tokenize and preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ef2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/kasparbeelen/.cache/huggingface/datasets/biglam___atypical_animacy/default/1.1.0/5827ff537a514460d4773100308d2bcc0bf867d323c3c472e5a506784da84fba/cache-66a796427ff630d3.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57aa6fece1fd4f2b87a8befc18a03283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': 'sentence'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2dfb65",
   "metadata": {},
   "source": [
    "## Train Model on Annotated Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86c1c1cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PartialState' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../results\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     seed \u001b[39m=\u001b[39;49m \u001b[39m42\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kasparbeelen/Documents/dhoxss-text2tech/Sessions/5b-PLMs.ipynb#Y155sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1333\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1328\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1334\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1335\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1336\u001b[0m ):\n\u001b[1;32m   1337\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1340\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1344\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1350\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1697\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1697\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1631\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_gpu \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1630\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m PartialState(backend\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddp_backend)\n\u001b[1;32m   1632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_gpu \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PartialState' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results\", \n",
    "    seed = 42,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0ba23",
   "metadata": {},
   "source": [
    "### Evaluate on test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "predictions = trainer.predict(test_set)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "f1_score(preds,predictions.label_ids,average='binary')\n",
    "f1_score(preds,predictions.label_ids,average='macro')\n",
    "f1_score(preds,predictions.label_ids,average='micro')\n",
    "accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b90765",
   "metadata": {},
   "source": [
    "The model only returns logits by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543407e",
   "metadata": {},
   "source": [
    "## The feature extraction pipeline\n",
    "\n",
    "Here we will see how to get vectors for words in context.\n",
    "\n",
    "Similarly to what we did with word2vec, we may also want to have access to the vector of a certain word. However, unlike with word2vec, the vector of a word will depend on the context in which the word occurs. This means that we can't just ask for the vector of the word \"apple\", for example: we will need to ask for the vector of the word \"apple\" given a certain context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1ed2f",
   "metadata": {},
   "source": [
    "We first import the following two libraries, which will help us work with vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3173bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # python library used for working with vectors\n",
    "from scipy import spatial # package to help compute distance or similarity between vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187e7db",
   "metadata": {},
   "source": [
    "The pipeline task to obtain the vectors for tokens in a sequence is `feature-extraction`. As you can see, creating this pipeline is very similar to creating the `fill-mask` pipeline.\n",
    "\n",
    "We will store the pipeline in a variable called `nlp_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead190c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_features = pipeline(\"feature-extraction\",\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50081723",
   "metadata": {},
   "source": [
    "Given a sentence, the pipeline tokenizes the input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"They were told that the machines stopped working.\"\n",
    "\n",
    "output = nlp_features(sentence)\n",
    "output_vectors = np.squeeze(output) # This removes single-dimensional entries (i.e. for vector readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e3bd0",
   "metadata": {},
   "source": [
    "Let's inspect the output. First of all, let's print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed0646",
   "metadata": {},
   "source": [
    "This is an array (a list of vectors). Let's see its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors.shape) # Print the shape of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642bac26",
   "metadata": {},
   "source": [
    "This means that we have an arrray (in other words a matrix, a table) that has 11 vectors of length 768 (or, in other words, 11 rows with 768 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c09e73",
   "metadata": {},
   "source": [
    "**Question:** 11 vectors? Why 11?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0818f",
   "metadata": {},
   "source": [
    "Let's see how the sentence is tokenized (we've seen how above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the **SAME** tokenizer used in the pipeline:\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Encode the sentence into a sequence of vocabulary IDs\n",
    "encoded_seq = our_tokenizer.encode(sentence)\n",
    "print(encoded_seq)\n",
    "\n",
    "# And get the tokens given the vocabulary IDs\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)\n",
    "\n",
    "# And print the length of the tokenized sequence:\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89fde7",
   "metadata": {},
   "source": [
    "As you can see, the input sentence has been tokenized into 11 tokens. So what we have in the above array is 11 vectors (each one representing a word in the context of the sentence, **keeping the order of tokens**, i.e. the first vector will correspond to the special token `[CLS]`, the second vector to the token `the`, and so on until the last vector, which corresponds to the special token `[SEP]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82deecb9",
   "metadata": {},
   "source": [
    "How do we get the vector of a specific token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[6]) # The 6th element in the tokenized sentence is the token `machine` (we start counting from zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_vectors[6]) # Therefore, o the 6th vector in output_vectors is the vector of `machine` in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5879f",
   "metadata": {},
   "source": [
    "‚úèÔ∏è **Exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ee168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two `feature-extraction` pipelines, one for the  1760-1850 model, and\n",
    "# one for the 1890-1900 model. Find whether the cosine similarity between words\n",
    "# in sequences change depending on which BERT model you use.\n",
    "# \n",
    "# Type your code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
