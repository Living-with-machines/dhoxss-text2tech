{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: using pre-trained models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Introduced in 2013, Word2Vec had a huge revolutionary impact in natural language processing and its applications.\n",
    "\n",
    "Word2Vec is a technique that provides ways for computing **word vectors** (also called **word embeddings**).\n",
    "\n",
    "**Word vectors** are numerical representations of words (i.e. words represented as a sequences of numbers). These are created (trained, learned) from a corpus of texts. The set of word embeddings created from a corpus is called a model.\n",
    "\n",
    "**Intuition:** if the algorithm can guess correctly which words fit a certain context, it has some understanding of their meanings. So the model encodes contexts of use. But can we say word vectors represent a word's meaning?\n",
    "\n",
    "A word2vec model that can be shared and used out-of-the-box is called a **pre-trained model**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using a pretrained w2v model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a popular library called Gensim to load Word2Vec models and explore vector spaces.\n",
    "\n",
    "First, we import one of the most relevant modules from the `gensim` library: `KeyedVectors`.\n",
    "\n",
    "Documentation: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: downloading pre-trained embeddings with Gensim\n",
    "\n",
    "Gensim allows us to directly download and use some pre-trained models off-the-shelf ([see more info here](https://github.com/RaRe-Technologies/gensim-data#models)).\n",
    "\n",
    "Some of these models are very large, and can take long to download and load. We will use smaller word2vec models for demonstration purposes.\n",
    "\n",
    "We can import the `downloader` module from Gensim and use it as follows to download one of the available pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Gensim `downloader`:\n",
    "from gensim import downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a pretrained word2vec model and load it into a variable called `wiki_vectors`:\n",
    "wiki_vectors = downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to always know the data type:\n",
    "type(wiki_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyedVectors allow word vectors to be accessed as if from a dictionary:\n",
    "print(wiki_vectors[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyedVectors allow word vectors to be accessed as if from a dictionary:\n",
    "print(wiki_vectors[\"pear\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: loading a model from elsewhere"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec models are trained and shared all the time, and most of them are not available through the Gensim downloader. In fact, not all of them are easy to find, sometimes it requires doing a bit of archeology to find whether something you would find useful already exists and has been shared. For example, it's common for researchers to train their own models and upload them to Zenodo or another research repository. Or different institutions may have their own repositories.\n",
    "\n",
    "For example, you can find word vectors in many different languages [here](https://fasttext.cc/docs/en/crawl-vectors.html#models):\n",
    "> E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages. I Proceedings of the International Conference on Language Resources and Evaluation, 2018."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models often come as:\n",
    "* either a vectors-only file (i.e. just a file containing the word and associated vector),\n",
    "* or as full models (we will see this in the next notebook).\n",
    "\n",
    "Most pretrained models are shared as a vectors-only file, full models are generally kept only if one may update the model at some point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a file with vectors only, we use the `KeyedVectors` module from `gensim.models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import KeyedVectors from gensim:\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will use a Word2Vec model that was trained on a 4.2-billion-word corpus of 19th-century British newspapers.\n",
    "\n",
    "These embeddings were trained to investigate semantic change in the lexicon of mechanization in 19th-century British newspapers, by Nilo Pedrazzini and Barbara McGillivray:\n",
    "> Pedrazzini, Nilo & Barbara McGillivray. 2022. Diachronic word embeddings from 19th-century British newspapers [Data set]. Zenodo. DOI: https://doi.org/10.5281/zenodo.7181682\n",
    "\n",
    "Download the embeddings file from [here](https://zenodo.org/record/7181682). Download just one of the `.txt` files, put it in the `models/` folder, which should be inside `Sessions/` (create the `models` folder if needed).\n",
    "\n",
    "You can then use the `.load_word2vec_format()` method to load the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 1860s embeddings into a new variable `victorian_vectors`:\n",
    "victorian_vectors = KeyedVectors.load_word2vec_format('models/1860s-vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of `victorian_vectors`:\n",
    "print(type(victorian_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding for \"apple\":\n",
    "print(victorian_vectors[\"apple\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to compare the \"apple\" embeddings from these two different models: they are completely different!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, on their own, are pretty useless. Also, they are not comparable between different models (unless you align them, but that's for another time!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings make sense relatively to each other within the same model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the vector space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, what is a vector and how does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for the word \"apple\":\n",
    "apple_vector = wiki_vectors[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the content of the variable `apple_vector`:\n",
    "print(apple_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length of `apple_vector`:\n",
    "len(apple_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the data type of this?\n",
    "type(apple_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A numpy array ([numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)) represents a multidimensional array of fixed-size items. When it only has one dimension, it looks very similar to a list, [but still is different!](https://numpy.org/doc/stable/user/absolute_beginners.html#whats-the-difference-between-a-python-list-and-a-numpy-array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vocabulary of the model is the set of unique words in the model. Often, when training a model, a strict limit to the vocabulary is given, only keeping the top most common words or removing words which occur less than a number of times in the whole corpus.\n",
    "\n",
    "The size of the vocabulary can be found using the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the size of the vocabulary of model trained on 19thC English newspapers:\n",
    "print(len(victorian_vectors))\n",
    "\n",
    "# Find the size of the vocabulary of model trained on modern data:\n",
    "print(len(wiki_vectors))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect which words are part of the model. You can get them with `.index_to_key`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words from the model, store them in variable `w2v_words`:\n",
    "w2v_words = list(victorian_vectors.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five words of the vocabulary:\n",
    "print(w2v_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the word \"DHOxSS\" is part of the vocabulary:\n",
    "print(\"DHOxSS\" in w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the word \"Oxford\" is part of the vocabulary:\n",
    "print(\"Oxford\" in w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, then check if the word \"oxford\" is part of the vocabulary:\n",
    "print(\"oxford\" in w2v_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decisions involved when training a model: how many words do we keep? do we lower-case the corpus? and many many more.\n",
    "\n",
    "It is hard to define when 'large' is large enough. As a (very) rough rule of thumb, perhaps **at least** around 500K words are necessary to train embeddings on corpora of a very specific style/genre/topic, and 2-4 million words for somewhat more diverse sources. Ultimately, there is no 'right' number, and perhaps more is not necessarily better (or maybe it is... depending on your corpus and its quality). A good read on the topic is [this one](https://aclanthology.org/W17-6908.pdf).\n",
    "\n",
    "Note that the choice of the model and training decisions will have a huge impact on the word vectors you'll get."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity, relatedness, analogy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the advantage of representing words as vectors?\n",
    "\n",
    "We can calculate the similarity between two vectors. This is usually done with `cosine similarity`, which Wikipedia defines as \"the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths\".\n",
    "\n",
    "When we calculate the cosine similarity between word vectors, behind the scene we are actually ranking words relatively to one another depending on the size of the angle between any given word vector and the vectors of the other words.\n",
    "\n",
    "![](images/w2v.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's KeyedVectors module allows us to find similarities, relatedness and analogies between words.\n",
    "\n",
    "Let's see some useful functions:\n",
    "* `doesnt_match()`\n",
    "* `most_similar()`\n",
    "* `similarity()`\n",
    "* `n_similarity()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `.doesnt_match()` method\n",
    "\n",
    "Given a list of words, it guesses which element is semantically different from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_vectors.doesnt_match([\"oxford\", \"carrot\", \"pear\", \"zucchini\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `.most_similar()` method\n",
    "\n",
    "Given a word, `.most_similar()` returns the closest neighbours to the query word. The argument `topn` determines the number of neighbours to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: The top 10 most similar words to 'oxford':\n",
    "wiki_vectors.most_similar(\"oxford\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: The top 10 most similar words to 'carrot':\n",
    "wiki_vectors.most_similar(\"carrot\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: The top 10 most similar words to 'apple':\n",
    "wiki_vectors.most_similar(\"apple\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: The top 10 most similar words to 'apple', but using one of the Victorian models:\n",
    "victorian_vectors.most_similar(\"apple\", topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `most_similar()` method can also be used to perform some vector arithmetics. Since word vectors are vectors, we can do maths with them and observe interesting patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic: what word is to _woman_ the way _king_ is to _man_? We can try to figure this out by simple additions and subtractions:\n",
    "\n",
    "`woman + king - man = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or also:\n",
    "\n",
    "`man + queen - woman = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_vectors.most_similar(positive=['man', 'queen'], negative=['woman'], topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is often used to find analogies. What is to Paris the way England is to London? Or:\n",
    "\n",
    "`paris + england - london = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_vectors.most_similar(positive=['paris', 'england'], negative=['london'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `.similarity()` method\n",
    "\n",
    "This method allows you to find the similarity between words...\n",
    "\n",
    "(... actually, remember that this is the similarity between the vector representation of two words learned from a specific corpus, **HUGE DIFFERENCE!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between two words:\n",
    "wiki_vectors.similarity(\"bush\", \"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between two words:\n",
    "wiki_vectors.similarity(\"trump\", \"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, that will change if we use one of our Victorian models:\n",
    "victorian_vectors.similarity(\"bush\", \"president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, of course:\n",
    "victorian_vectors.similarity(\"obama\", \"president\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `n_similarity()` method\n",
    "\n",
    "This method computes cosine similarity between two sets of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wiki_vectors.n_similarity([\"oxford\", \"university\", \"summer\", \"school\"], [\"learning\", \"oxfordshire\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wiki_vectors.n_similarity([\"oxford\", \"university\", \"summer\", \"school\"], [\"sun\", \"beach\", \"holidays\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the the texts the vectors have been trained on, the respective vectors tend to <u>**reflect the bias they have acquired in the natural language**</u> as is represented in the corpus. Bias, of course, can be reflected across several variables (sexual orientation, ethnicity, political leaning, etc.).  \n",
    "\n",
    "Depending on the task, **this can be an important ethical issue**: it may not be if your model is meant to capture the semantics of words from a specific historical period, but it may be so if the vectors are used to improve certain recommendation systems, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is to woman the way doctor is to man?\n",
    "wiki_vectors.most_similar(positive=['woman', 'doctor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is to man the way doctor is to woman?\n",
    "wiki_vectors.most_similar(positive=['man', 'doctor'], negative=['woman'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings have been used in research to quantify biases and stereotypes, for example in [this famous PNAS paper](https://www.pnas.org/doi/full/10.1073/pnas.1720347115)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise:**\n",
    "\n",
    "Load a different model (in a different language, for example) and explore similarities between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment and type your code here:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise:**\n",
    "\n",
    "With the model you have loaded, can you identify any biases inherited from the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment and type your code here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
