{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEjIZfUkzA0f"
   },
   "source": [
    "# Word2Vec: training your own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how you can train your own Word2Vec model.\n",
    "\n",
    "There are two main steps:\n",
    "* Preprocessing the corpus you will train your model on.\n",
    "* Training the model.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yePUXhVizA0g"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCsjMvZCzA0h"
   },
   "source": [
    "Word2Vec requires the training data to be provided as an iterable (e.g. a list of tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different choices can be made when preprocessing a corpus to be used as Word2Vec training data. Ultimately the choices you'll make will usually depend on:\n",
    "\n",
    "1. The size of your corpus (it might be computationally too expensive to preprocess very large corpora: choose a quality-time trade-off wisely!)\n",
    "2. What you are planning to use the model for (e.g. maybe you want to look at non-lemmatized tokens? Maybe you'd like to keep numbers?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first complete example, our preprocessing will consist of:\n",
    "- Lowercasing\n",
    "- Punctuation removal\n",
    "- Stopword removal\n",
    "- Lemmatization\n",
    "\n",
    "But preprocessing may also include tasks like:\n",
    "- Removal of tokens with less than _n_ characters\n",
    "- Removal of numbers\n",
    "- PoS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are experts in spacy now, we will use it for preprocessing our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EogcuS4zA0h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_8LBHHozA0i"
   },
   "source": [
    "Load the English model `en_core_web_sm` from `spacy`:\n",
    "\n",
    "**Note:** If the following cell does not work, this is because you need to download the Spacy model again. You can do it typing the following in a new cell:\n",
    "```\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ30_GS5zA0i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BisNuqwSzA0i"
   },
   "source": [
    "The following function takes in a sentence and returns a list of tokens (i.e. words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwlrSItTzA0j",
    "outputId": "d371da99-249e-4840-e9cd-a988ff8e546a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function that tokenizes a sentence and applies whichever preprocessing you want:\n",
    "def tokenize_a_sentence(sentence):\n",
    "    sentence = sentence.lower() # Lowercase the sentence\n",
    "    doc = nlp(sentence) # Turn the sentence into a Spacy Doc object\n",
    "    list_tokens = [] # Instantiate an empty list, where we will store the tokens we want to keep.\n",
    "    for token in doc: # Iterate over the tokens in the sentence.\n",
    "        if token.is_stop == False and token.text.isalpha() == True: # Keep tokens that are not stopwords and consist only of alphabetic characters.\n",
    "            list_tokens.append(token.lemma_) # Add the token's lemma into the list of tokens.\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the function works with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = 'These are test sentences, just to have a look at how a processed sentence looks like.'\n",
    "\n",
    "# Apply the function to a new sentence:\n",
    "tokenized_sentence = tokenize_a_sentence(sentence)\n",
    "\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise 1:**\n",
    "\n",
    "Can you rewrite the `tokenize_a_sentence()` function as a list comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a dataset and preprocess it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy example, we will use the `text` column from a CSV file of 19th-century newspaper articles.\n",
    "\n",
    "We will use `pandas` library because it makes it will make it easier to extract a column from a CSV.\n",
    "\n",
    "We will then pre-process each text with our `tokenize_a_sentence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "jXvft6M_zA0k",
    "outputId": "e3b4f6b5-5c8e-46c1-8a7f-8b8d82f406e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data from a pandas dataframe:\n",
    "incsv = pd.read_csv('data/newspapers-toy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the size of the dataset:\n",
    "incsv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the first rows of the dataframe:\n",
    "incsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep the values of the `text` column in a new variable:\n",
    "training_set = incsv[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the content of training_set:\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply the preprocessing function to each sentence in the dataset.\n",
    "\n",
    "The output will be a list of lists, where each of the inner lists contains the relevant tokens in the text.\n",
    "\n",
    "We will use the `tqdm` to track the progress of the training (i.e. to show the progress bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # To track the progress of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yhul8-e5zA0l",
    "outputId": "ef524df0-1864-45c8-cf91-2443e21fc0bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = [] # Empty list: that's where we will store our training data.\n",
    "\n",
    "# Iterate over all texts in our training set:\n",
    "for text in tqdm(training_set):\n",
    "    \n",
    "    # Tokenise\n",
    "    tokenized_text = tokenize_a_sentence(text)\n",
    "\n",
    "    # Add the tokenized text (i.e. a list of tokens) to the list that will be\n",
    "    # used to train the model.\n",
    "    training_data.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the size of the `training_data` variable\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first element of `training_data` (i.e. the first text, tokenised):\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njMNMzf6zA0j"
   },
   "source": [
    "## Train a word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Word2Vec module from Gensim to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv-aO5ijzA0k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6jHNF_vzA0k"
   },
   "source": [
    "**Hyperparameters** are training parameters whose values can be decided by the user. Different choices of hyperparameters will impact the training.\n",
    "\n",
    "[Here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) is a breakdown of all possible hyperparameters which can be changed if we train a model using Gensim's `Word2Vec`. We'll mostly stick to the defaults (but spell out the most important ones for clarity).\n",
    "\n",
    "Hyperparameters are specified by the user when initialiasing a new Word2Vec model, before providing the data to learn from, and before starting the training.\n",
    "\n",
    "It's okay to start with the default ones (which you can find specified in the link just provided, e.g. min_count = 5), which correspond to the ones that were found by several studies to be optimal for many different tasks.\n",
    "\n",
    "Normally, it is good practice to find the **optimal parameters** for your model, or better still, for the purpose for which you plan to use your model.\n",
    "\n",
    "For example, a higher window of words, i.e. the parameter `window`, will make the model consider more words before and after each token when calculating the vector for each word in the corpus: this seems to lead the model to capture more semantic similarities/relationships, whereas a smaller window will tend to reflect similarities which are more syntactic in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instantiate Word2Vec (using the default parameters) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate Word2Vec with the default hyperparameters:\n",
    "w2v_model = Word2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can also specify your choice of hyperparameters in the brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59Hx0Tm-zA0l",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate Word2Vec with your own hyperparameters:\n",
    "w2v_model = Word2Vec(min_count=5, # how often a word should appear in order to be included\n",
    "                     window=5, # how many words before and after count as context\n",
    "                     sg=1, # using the SkipGram algorithm (1) or the CBOW algorithm (0)?\n",
    "                     vector_size=100, # Size of the vector\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl3XvycMzA0m"
   },
   "source": [
    "Before training, you will need to build the vocabulary. You can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KanyrdaMzA0m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(training_data) # Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B8-t3-LzA0m"
   },
   "source": [
    "We then train the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynvC7RdyzA0m",
    "outputId": "d0db0a11-f4bb-4b2a-835b-872d35612995",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model.train(training_data, # tokenised data\n",
    "                total_examples=len(training_data), # Number of sentences to use for training\n",
    "                epochs=5, #how many epochs to train for\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haIDuI-dzA0m"
   },
   "source": [
    "We will now save both the whole model and the vectors. This is because, while a file containing only the vectors is easier to manage, saving the full model as well allows you to update it in the future (e.g. to keep training it on more relevant sentences).\n",
    "\n",
    "Note: `binary=False` will simply save the vectors in non-binary format (i.e. human-readable), which can take longer to store and process, but easier to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM-ox6bOzA0m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model.save(\"models/test-model\") # Save the full model (in case we'd like to update it in the future)\n",
    "w2v_model.wv.save_word2vec_format(\"models/test-model-vectors.txt\", binary=False) # Also save the vectors only (easier to work with)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how to load the vectors-only file.\n",
    "\n",
    "Now, to load a full model, we will use the `Word2Vec` module from `gensim.models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To load the full model, we need to import Word2Vec from gensim:\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To read a word2vec model, use the .load() method, passing in the path to the model we just trained and saved:\n",
    "our_test_model = Word2Vec.load(\"models/test-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the data type:\n",
    "type(our_test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we load a full model like this, we **can't** access the embeddings in the same way we did with `KeyedVectors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_test_model[\"liverpool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can simply obtain the model's vectors as a `KeyedVectors` object by 'isolating' the vectors, with method `wv`, in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the model's vectors as a KeyedVectors object and store it in `our_test_vectors`:\n",
    "our_test_vectors = our_test_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the data type of the pretrained vectors in `our_test_vectors`:\n",
    "type(our_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the embedding of token `liverpool`:\n",
    "print(our_test_vectors[\"liverpool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the most similar to word \"liverpool\":\n",
    "our_test_vectors.most_similar(\"liverpool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise 2:**\n",
    "\n",
    "Obtain a corpus fo text that is interesting for your research (medieval chronicles, the Shakespeare sonnets, Don Quijote, etc.).\n",
    "\n",
    "Download it, preprocess it, train a Word2Vec model, explore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Type your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **Exercise 1:**\n",
    "\n",
    "Can you rewrite the `tokenize_a_sentence()` function as a list comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentence provided:\n",
    "sentence = 'These are test sentences, just to have a look at how a processed sentence looks like.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function provided:\n",
    "def tokenize_a_sentence(sentence):\n",
    "    doc = nlp(sentence) # Create Spacy Doc object\n",
    "    list_tokens = [] # Instantiate an empty list, where we will store the tokens we want to keep.\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.text.isalpha() == True: # Keep tokens that are not stopwords and are all alphabetic.\n",
    "            list_tokens.append(token.lemma_) # Add the lemma into the list of tokens.\n",
    "    return list_tokens\n",
    "\n",
    "# Calling the function:\n",
    "tokenized_sentence = tokenize_a_sentence(sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suggested answer, using a list comprehension:\n",
    "tokenized_sentence = [token.lemma_ for token in nlp(sentence) if token.is_stop == False and token.text.isalpha() == True]\n",
    "print(tokenized_sentence)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lE0rSC2VzA0r",
    "HEFmGiSTzA0t"
   ],
   "name": "word2vec_dhoxss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "54ed4be044bdbfddf339b8bedcd1bd34fa47dd0b44ee203d7e74423349b7c92b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
