{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c5706f",
   "metadata": {
    "id": "KVA77p00hTdo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5727cca",
   "metadata": {
    "id": "cdq1kREmjrwA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spacy is an open-source library for text preprocessing.\n",
    "\n",
    "It features the most common components of the preprocessing pipeline:\n",
    "\n",
    "![](images/nlp_pipeline.png)\n",
    "\n",
    "Spacy takes a string as input, i.e. a sequence of characters, and transforms this string into a sequence of more meaningful units for analyses. Which transformations we want to perform, again, will depend on the nature of our dataset, and the type of analysis we want to perform.\n",
    "\n",
    "Image source: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e050243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spacy is a python library and it has to be imported, to be able to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248758e",
   "metadata": {
    "id": "_gtE6MvSvwWR",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafb7d3",
   "metadata": {
    "id": "ng9KTcDj5VKr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spacy is based on statistical models that have learned probabilities from datasets richly annotated with linguistic features (more on that later!). A Spacy model has learned from observations in the data, so that we can then apply it to new text.\n",
    "\n",
    "Spacy provides easy-to-use pipelines for a variety of languages and for a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52acbe0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/spacy_langs.png)\n",
    "\n",
    "Source: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e417d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ☝️What if my language does not have any trained pipeline?\n",
    "\n",
    "Unfortunately, there's no toolkit that supports all languages in the world. What are the options then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bddef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NLTK**\n",
    "\n",
    "NLTK is the other main library for linguistic processing, also supporting many languages. This is the classical NLTK toolkit for natural language processing. We really recommend the [NLTK book](https://www.nltk.org/book/) to learn how to use it. NLTK and Spacy overlap a lot but they are somewhat complementary as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951db68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Language-specific packages**\n",
    "\n",
    "There are some language-specific libraries, which may work better for a specific language, especially for less-resourced languages, but they may be difficult to find. Also, each has their own way of using it, so you'd need to follow the documentation they provide. For example, `lamonpy` for latin: https://github.com/bab2min/lamonpy. Our suggestion is to get in touch with NLP experts in the specific languages (e.g. by browsing the [ACL anthology repository](https://aclanthology.org/), e.g. for [Coptic](https://aclanthology.org/search/?q=coptic))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391fcd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stanza**\n",
    "\n",
    "Recently, Spacy has added the option of using the [Stanza](https://stanfordnlp.github.io/stanza/) (StanfordNLP) research models directly in spaCy. Stanza has models in [66 languages](https://stanfordnlp.github.io/stanza/available_models.html). To use a Stanza model instead of a spaCy model, the only difference is in how the model/pipeline is downloaded or loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467a3e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training Spacy models**\n",
    "\n",
    "Spacy supports learning new languages or fine-tuning models to different domains. This is well-documented [here](https://spacy.io/usage/training). However, this requires annotated data, and it would be quite some work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace7797",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get started with spaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f95a7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the `en_core_web_sm` pipeline in our examples, which is trained on English data: https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e6cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to start using spacy, we need to download a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d483a43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4369de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "... And we need to import the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140b90d",
   "metadata": {
    "id": "iCjp6UrojvXJ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load English language pipeline and store in variable `nlp`:\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5e97f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `nlp` pipeline takes as input a text and runs it through the pipeline.\n",
    "\n",
    "![](images/nlp_pipeline.png)\n",
    "\n",
    "We store the output in a variable that we call `output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dc6b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"This is a great week. Is it not?\"\n",
    "output = nlp(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2631d25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you print the content of `output`, on the surface it looks like a string. It actually looks like nothing has happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc341e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"This is a great week. Is it not?\"\n",
    "output = nlp(example)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed06c99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if we print the data type of `output`, we see it's not a string, it's a spacy object called `Doc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3277f38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print the type of `output`:\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe0910",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A [Doc](https://spacy.io/api/doc) contains the linguistic annotations as a sequences of linguistic units.\n",
    "\n",
    "In other words, variable `output` now contains the linguistic processing of your sentence!\n",
    "\n",
    "![](images/nlp_pipeline.png)\n",
    "\n",
    "So all our efforts will now be put into retrieving the linguistic information from the output ([full documentation here](https://spacy.io/usage))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55cdde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, to recap, that's how to process a text using Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b3a19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load English language pipeline and store in variable `nlp`. You may need to download it first,\n",
    "# and you only need to do this step once!\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# The text to process:\n",
    "example = \"This is a great week. Is it not?\"\n",
    "\n",
    "# Process the example text using the pipeline stored in `nlp`, and\n",
    "# store the output in a variable called `output`. This line does most\n",
    "# of the work!!!\n",
    "output = nlp(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70226df5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, iterating over the elements in doc allows you to retrieve its linguistic information.\n",
    "\n",
    "In particular, a [`Doc`](https://spacy.io/api/doc) object is a sequence of [`Token`](https://spacy.io/api/token) objects (i.e.~words). Iterating over the elements in a `Doc` object means iterating over its tokens. More interestingly, you will be able to access the token attributes, listed in https://spacy.io/api/token#attributes, using \"dot notation\". For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f9222",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"This is a great week. Is it not?\"\n",
    "output = nlp(example)\n",
    "\n",
    "# Iterating over the elements in `output`, using a for-loop:\n",
    "for element in output:\n",
    "    # According to the documentation, .text provides the \"verbatim text content\" of a token. \n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820e6ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "☝️It is common to use a list comprehension instead of a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fac1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"This is a great week. Is it not?\"\n",
    "output = nlp(example)\n",
    "\n",
    "# Iterating over the elements in `output`, using a list comprehension:\n",
    "print([element.text for element in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a51c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ✏️ Exercise:\n",
    "\n",
    "1. Download a pipeline in your language of choice.\n",
    "2. Load the trained pipeline into a variable.\n",
    "3. Use the pipeline to process a text, iterate over its tokens, and return the verbatim text content of each token.\n",
    "4. Instead of returning the verbatim text content of each token, return its lemma, using the attribute `.lemma_`. See documentation here: https://spacy.io/api/token#attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3107d79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "👀 **Using the `stanza` wrapper:**\n",
    "\n",
    "To use the Stanza models as a Spacy pipeline, instead of **downloading and loading the pipeline** as shown above, you'll need to do the following, with the correct [language code](https://stanfordnlp.github.io/stanza/available_models.html), like this, e.g. for Greek (`el`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "stanza.download(\"el\")\n",
    "nlp = spacy_stanza.load_pipeline(\"el\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a5edd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Download a pipeline in your language of choice. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad38d4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load the trained pipeline into a variable. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b6aa6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Use the pipeline to process a text, iterate over its tokens, and return the verbatim\n",
    "# text content of each token. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f2e33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Instead of returning the verbatim text content of each token, return its lemma, using\n",
    "# the attribute `.lemma_`. See documentation here: https://spacy.io/api/token#attributes.\n",
    "# Type your code here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2e91717858118300f159f516e8add62a50cea7986ea1e6059fd0b766f06d37e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
