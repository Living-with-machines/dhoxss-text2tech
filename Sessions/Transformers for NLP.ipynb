{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1ca93",
   "metadata": {
    "id": "cyF-X7Ho0E--"
   },
   "source": [
    "# 1. Static word embeddings\n",
    "\n",
    "Introduced in 2013, word2vec has had a huge impact in natural language processing and its applications.\n",
    "\n",
    "Vector representations of words seem to capture word meaning quite well!\n",
    "\n",
    "Accessible and easy to use (easy to train, to apply and to share).\n",
    "\n",
    "Shortcoming: this algorithm creates static embeddings, i.e. it creates one vector per word, no matter how many meanings the word has (e.g. `I like apples` vs `I like Apple macbooks`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488d1d6",
   "metadata": {
    "id": "1yVysUSK0yI8"
   },
   "source": [
    "Import the `gensim` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e03bc8",
   "metadata": {
    "id": "IcAc17ZA01yc"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be84ef3f",
   "metadata": {
    "id": "eiiasj951mOB"
   },
   "source": [
    "Download and load one of the models.\n",
    "\n",
    "Just for illustration, we'll use `glove-wiki-gigaword-50`, which was trained on text from Wikipedia and Gigaword (newswire). Note that different models may perform differently, as we showed this morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56024d20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVR6arbK15D7",
    "outputId": "4e138515-7f72-4c4b-b789-9a12908b0d2d"
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8618b",
   "metadata": {
    "id": "feiy-OWs3Au-"
   },
   "source": [
    "Static word embeddings create one vector per word.\n",
    "\n",
    "_Example 1:_\n",
    "See top 20 most similar words to word 'mouse'. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccc4fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1CWM5FQ2NFl",
    "outputId": "123fbf85-c03b-4733-8ea0-6ae4bdd28d70"
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('mouse', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e8c41",
   "metadata": {
    "id": "xWzLG0mC3d9J"
   },
   "source": [
    "_Example 2:_ See top 20 most similar words to word 'pear'.\n",
    "\n",
    "What would you expect to see here? Any guesses on the top most similar? And what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07003933",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kft1MBsW2wRO",
    "outputId": "42759d8b-d8a3-4e51-ba92-3f19b57b7f8b"
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('pear', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41bae4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zik2L8-X2z0y",
    "outputId": "26b82866-7950-4b19-e204-4cda9c260cc1"
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('apple', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb1d8f",
   "metadata": {
    "id": "FzvqY_p20jj9"
   },
   "source": [
    "# 2. Contextualized word embeddings\n",
    "\n",
    "Words mean different things in different contexts.\n",
    "\n",
    "Goal: learn the representation for each word in its context. In other words, capture what a word means in its context.\n",
    "\n",
    "In recent years (since 2018 mostly), lots of progress has been made (Google's BERT and OpenAI's GPT-2 and GPT-3).\n",
    "\n",
    "Also, lots of progress in making this easily accessible, and easy to use. The company HuggingFace has been greatly responsible for this last point, especially with their `transformers` library and their model hub. However, it is still not as straightforward to use (especially training) as word2vec is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74befc7",
   "metadata": {
    "id": "fdAWG8oKZ6AW"
   },
   "source": [
    "A **transformer** is a deep learning model that uses the **attention** mechanism (a mechanism which is based on cognitive attention, and which focuses on where the key information in a sequence is produces while forgetting less relevant information). Its development has had a huge impact in deep learning, especially in natural language processing and computer vision. It allows a more effective modeling of long term dependencies between the words in a sequence, and more efficient training, not limited by the sequence order of the input sequence.\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) is a transformer-based model that creates contextualized word embeddings. It learns contextualized information through a masking process (i.e. it hides some words and uses their position to infer them back)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86effad",
   "metadata": {
    "id": "mnbqG479I1ag"
   },
   "source": [
    "We will first install the `transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21795fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-Xfm8H1I4BQ",
    "outputId": "2e550828-c38b-459e-f3d5-cf677d9cf1a7"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80006c",
   "metadata": {
    "id": "ZV5IsTTgZlVX"
   },
   "source": [
    "Import the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab83eb",
   "metadata": {
    "id": "P6wepbWPZjQy"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5417",
   "metadata": {
    "id": "leCcO-ZoRchV"
   },
   "source": [
    "### 2.1. Using BERT pipelines\n",
    "\n",
    "Pipelines are a simplified way to apply BERT models. A pipeline is a code object that abstracts most of the complex code (it happens in the background), leaving only the bare minimum for the user to interact.\n",
    "\n",
    "We load the `pipeline` module from the `transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bce262",
   "metadata": {
    "id": "vs1_41BRPKgD"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a8600",
   "metadata": {
    "id": "38gnrw2gNVbs"
   },
   "source": [
    "To create a pipeline, you need to know:\n",
    "* Which task you want to perform (e.g. `'fill-mask'`)\n",
    "* The model you want to use to make predictions (e.g. `'distilbert-base-uncased'`), which must be trained for the task you want to perform (i.e. `fill-mask`).\n",
    "* The tokenizer used by the model (i.e. the strategy that BERT uses to split sequences into smaller units. This is often the same name as the model, e.g. `'distilbert-base-uncased'`).\n",
    "* Which conventions the language model follows: e.g. if your task is `fill-mask`, how is the masked element tagged (usually `[MASK]`, sometimes `<MASK>`, etc.).\n",
    "\n",
    "If you obtained your model from the HuggingFace model hub (https://huggingface.co/models), you should be able to find all this info in the model card (e.g. https://huggingface.co/bert-base-uncased).\n",
    "\n",
    "**Note:** You can find more information on pipelines and how to use them in https://huggingface.co/transformers/main_classes/pipelines.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0aa93",
   "metadata": {
    "id": "wHftD07XWP8j"
   },
   "source": [
    "### 2.2. The Mask filling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d97b1",
   "metadata": {
    "id": "Fw-gYTxGToyX"
   },
   "source": [
    "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token (source: https://huggingface.co/transformers/task_summary.html#masked-language-modeling). The `fill-mask` pipeline replaces the mask in a sequence by the most likely prediction according to a BERT model.\n",
    "\n",
    "We will create a `fill-mask` pipeline using the `distilbert-base-uncased` English model (and its tokenizer), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010f77a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "referenced_widgets": [
      "c4127d0a8863429baca65151680237fb",
      "07ed00183908469f89f0714c94b8be66",
      "892489a1a22d452eb2b8017231a49f63",
      "fe6c962147dd4250b8099cca88c4e82b",
      "ee0fd2e49c694c5cace010298133b86e",
      "5d3715e3b70c4b3aa95a95d157f7ba3a",
      "f0c092ac58e44e1fb62f51eb301339ef",
      "73ae6c09260a4101af78e91a2f67ee2f",
      "248b91fa193f43cd912431e6034116cd",
      "9ab77b5d3cad496a994d549fd84fcabe",
      "24487affaded4b24905b4398280cc16c",
      "ccfafaf56b1443d99b50308a5dd6c16d",
      "47ba9e6233e4448faa9435907c8a5589",
      "27ee1fc1ea4f49c2b769d3c0cb92fbcd",
      "605619be5e514845a5b5ab2a0319cb23",
      "96f7c81bce7f43449e1c2e17c083be1b",
      "537fe636ee6f40afa33d04a2e340311b",
      "a45ec38265ec4995927d545c9b12eddd",
      "90308ebc135a4777b26b75ce2a9948a8",
      "8460c6841339421580488018abff4d92",
      "cca00ac611bd4765a5d78d8001d0a4c3",
      "bda05c808a084aea9521d1925895890d",
      "88f64238a98e4f8f9bfb4c39abad112b",
      "fc631c664369444f96b141e235ed9a01",
      "8ba8bc7234534f84b4be792131473e2e",
      "7b90bd58f453439a83ce9ae244f3c844",
      "0e3665460a7c4f27a31fb1f2a026e52e",
      "878e15bc14e24bfd969dfa0bafe57a46",
      "34b9668e352546e7a48e2f7ef01b6ece",
      "58dfdaf7f38d4c60926b958ed8702329",
      "fa36fe68f2504d22a022302277f74e83",
      "37948521405c4f9eadb7ad71ca677474",
      "1ad2a42bc785434aa0a1186f42345f37",
      "1b0bece128bb4490aac5da3fc465f08f",
      "47bc5ae05b0a4458ba37f4f7fa3d933d",
      "f69252a6efd34300825ecf215e4b5db0",
      "f24dd8ac049c48d397032e228134f82d",
      "42f55e703d9d45a5abc6dd3883b3e0c3",
      "418a65725a6a4ccf9df83c086b84d1a7",
      "778718ab1d1540468af783745300ecc6"
     ]
    },
    "id": "QExSFUJuPKiX",
    "outputId": "20bddc4d-9db7-4d4a-ceec-13b53719ef04"
   },
   "outputs": [],
   "source": [
    "unmasker = pipeline('fill-mask',\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f2a44",
   "metadata": {
    "id": "75GM6hUbUBXc"
   },
   "source": [
    "This pipeline allows us to easily use BERT to predict the masked element in a sentence.\n",
    "\n",
    "In the previous cell, we are:\n",
    "* Creating a pipeline for the task of `fill-mask`,\n",
    "* by using the `distilbert-base-uncased` BERT model and tokenizer,\n",
    "* and storing the resulting pipeline in a variable (we call it `unmasker`), which we can use and reuse in subsequent code.\n",
    "\n",
    "**Warning:** You need to make sure the model you use is trained for the `'fill-mask'` task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aab03b",
   "metadata": {
    "id": "Kg4Ta73vSFY3"
   },
   "source": [
    "To use the pipeline, you just need to pass the sentence containing the masked word as an argument of `unmasker` (i.e. the variable containing your pipeline). You don't need to do any encoding, the pipeline already takes care of converting the text into an input BERT can understand!\n",
    "\n",
    "We store the output of applying the pipeline to this sentence in the `outputs` variable, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f61830",
   "metadata": {
    "id": "AEBgbIZSR_UH"
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"The cell is guarded by a [MASK].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da12aa9",
   "metadata": {
    "id": "3BFrn3ZqSVg4"
   },
   "source": [
    "Now, let's inspect the `outputs` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQYwNfXQSoHg",
    "outputId": "bedb5c97-863d-483b-a7b5-f0c9ac96bd3a"
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723d43b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxBd5fa3TXhm",
    "outputId": "667ccdac-26f6-4dbe-8ad4-2bbcaf8f6379"
   },
   "outputs": [],
   "source": [
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed480a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5UZFsqzcZvt",
    "outputId": "c01331e0-fca6-43b8-eaa9-985f0d60d2d5"
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"\"\"When a cell has been produced, we can then trace some of the\n",
    "                      stages by which new [MASK] are formed. There appear to be four\n",
    "                      modes in which vegetable cells are multiplied. The new cells\n",
    "                      may either proceed from a nucleus or they may be formed at\n",
    "                      once in the protoplasm.\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730a372",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2Ia8CvZdEXV",
    "outputId": "0e8c960f-d059-4e2c-9a9b-d2860553a654"
   },
   "outputs": [],
   "source": [
    "outputs = unmasker(\"\"\"Imprisonment with proper employment, and at least two visits\n",
    "                      every day from a prison officer. The punishment does not\n",
    "                      extend over a month. A week must elapse before the same\n",
    "                      prisoner can be put again into the dark [MASK].\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed830b",
   "metadata": {
    "id": "PCx9f4pzZpTo"
   },
   "source": [
    "### 2.3. BERT Tokenizer\n",
    "\n",
    "As we saw this morning, tokenizing a text is splitting it into meaningful units and converts these units into numbers  that the model can make sense of.\n",
    "\n",
    "BERT uses a subword tokenization procedure called WordPiece.\n",
    "\n",
    "This means that it does not only separate words. It also splits certain words into meaningful units\n",
    "> E.g. it splits the word `tokenizing` into `token` and `##izing`, where `##` indicates that this is a suffix which should be attached to the previous word).\n",
    "\n",
    "The tokenizer maps every word form (e.g. `token` and `##izing`) with IDs in the vocabulary (e.g. given a certain model, `19204` is the vocabulary ID of `token` and `6026` is the vocabulary ID of the suffix `##izing`).\n",
    "\n",
    "**!!! Warning:** BERT has certain limits as to the length of the string that is accepted, often 512 tokens.\n",
    "\n",
    "**!!! VERY important:** different models may have different token-to-id mappings. When we use an existing model, we must use the same tokenizer (and therefore the same vocabulary mapping) that was used when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7cd2a",
   "metadata": {
    "id": "-P5gKh2GqWS4"
   },
   "source": [
    "#### The inner workings of BERT tokenization\n",
    "\n",
    "Tokenization steps:\n",
    "\n",
    "1. The text is split into tokens, which can be:\n",
    "  * words\n",
    "  * parts of words\n",
    "  * punctuation symbols\n",
    "\n",
    "2. The tokenizer adds special tokens:\n",
    "  * `[CLS]` indicating that this is the beginning of the input sequence.\n",
    "  * `[SEP]` indicating that it is the end of the sequence (or a sequence delimiter if we have a pair of sequences as input).\n",
    "\n",
    "3. The tokenizer maps each token into their vocabulary IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef51fa",
   "metadata": {
    "id": "mQFYLklrpQRz"
   },
   "source": [
    "Let's explore this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40798eca",
   "metadata": {
    "id": "RthE6sQiZXqh"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer of a certain BERT model\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c92d8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRczZfPRZXs3",
    "outputId": "8590811c-1611-4e7e-8777-7b73899ebf99"
   },
   "outputs": [],
   "source": [
    "# The `encode` function does the three steps in one go (splits the sequence, adds\n",
    "# special tokens, and converts them into a sequence of IDs):\n",
    "encoded_seq = our_tokenizer.encode('The cell is relentlessly guarded by guards.')\n",
    "print(encoded_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5025d9",
   "metadata": {
    "id": "_E_CFHlvNIfD"
   },
   "source": [
    "And there are also functions that translate the vocabulary IDs to the word forms (given a certain tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffeef6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRuw2R-2ZXxT",
    "outputId": "144a8bd2-add2-46e3-a96c-8964e7b22266"
   },
   "outputs": [],
   "source": [
    "# The `convert_ids_to_tokens` returns the tokens that correspond to the IDs of\n",
    "# an encoded sequence:\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc97af",
   "metadata": {
    "id": "gnE7bkL3sAoB"
   },
   "source": [
    "### 2.4. Exercise\n",
    "\n",
    "Find a `fill-mask` model from HuggingFace model hub (trained on data in your preferred language, if there is one). Create a `fill-mask` pipeline and try to predict the mask token in some sentences.\n",
    "* Try this with different sentences.\n",
    "* What do the scores indicate?\n",
    "* Try to see what happens if you want to use BERT to predict something that requires world knowledge, for example:\n",
    "\n",
    "  * `Everyone agrees that the princes in the tower were [MASK].`\n",
    "  * `It would seem [MASK] III killed the princes in the tower.`\n",
    "  * `Barcelona is a city in [MASK].`\n",
    "  * `Paris is the capital of [MASK].`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4db19",
   "metadata": {
    "id": "7fGFFUVmsNrQ"
   },
   "outputs": [],
   "source": [
    "# Type your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9b42b",
   "metadata": {
    "id": "3itPLpg4zoO2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a9e5834",
   "metadata": {
    "id": "t--wEQzKvfls"
   },
   "source": [
    "### 2.5. Load and use your own models\n",
    "\n",
    "In this tutorial we won't have time to cover how to train or fine-tune your own BERT model, but at the end of this notebook you will find some links on this.\n",
    "\n",
    "We will now imagine you have your own BERT models you want to use. Instead, we will be using our historical English BERT models, just to show that you can also use the `transformers` library using your own model. You just need to correctly point the right path to the model when loading it.\n",
    "\n",
    "See how we load our historical English BERT models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8fdf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zq6k6lFtnGfE",
    "outputId": "5ded51dd-2cd7-4966-de4c-4f36e76bf67c"
   },
   "outputs": [],
   "source": [
    "# We have stored our historical English BERT models stored in Google drive, in\n",
    "# https://drive.google.com/drive/folders/1Y-ltpJNCfTO0ti7zPnBdRWlyMXh8OjmH?usp=sharing.\n",
    "# These language models are described in https://arxiv.org/abs/2105.11321.\n",
    "#\n",
    "# !!! Important facts you will **need to know** about these language models:\n",
    "# * They were fine-tuned on the `fill-mask` task based on `bert-base-uncased`\n",
    "# * They use the `bert-base-uncased` tokenizer.\n",
    "#\n",
    "# The dataset on which these language models are trained is a 19th-century collection\n",
    "# of books in English. We will download the following two BERT models:\n",
    "# * bert_1760_1850.zip: trained on books from 1760 to 1850: https://drive.google.com/file/d/1QJgUFiFgplOq2eBUn5mLwAxcn3KOSPxw/view?usp=sharing\n",
    "# * bert_1890_1900.zip: trained on books from 1890 to 1900: https://drive.google.com/file/d/1nPlcyBBOdGYxRGVmiCrgC6muhgD87lva/view?usp=sharing\n",
    "#\n",
    "# With the following commands, I am directly downloading the .zip files to colab\n",
    "# (as Kaspar has already shown):\n",
    "!gdown --id 1QJgUFiFgplOq2eBUn5mLwAxcn3KOSPxw\n",
    "!gdown --id 1nPlcyBBOdGYxRGVmiCrgC6muhgD87lva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893296e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miDz-JUGnGha",
    "outputId": "bfd7336b-7192-4172-aac1-63bfcb3cf551"
   },
   "outputs": [],
   "source": [
    "# We then unzip the downloaded files:\n",
    "!unzip bert_1760_1850.zip\n",
    "!unzip bert_1890_1900.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b28f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "d0b22839bf2e443995e167b46eac6069",
      "9098c0ae1fc4423d92964a6e50a013f6",
      "d287a9759eb849e3823f4ebf42db3b53",
      "144958ab732c4cdfa23b9034aebc6efc",
      "a5faa6fb33e542669d3877f03442292a",
      "5f6d7a5cc9c04db1b65202e4314b8323",
      "d527ecb6504149e69b9503d29d194a02",
      "a13dbcf9ae50468dae5ef14dd80872f9",
      "9efe21c6002640edb3ebd9805de32798",
      "3eae2c6fd4f8467eb6e2279873b2ea1a",
      "81a8108e5c4c4590b1955ce64edc8ac1",
      "3562bc738d9043ba92501eff95f2bbd8",
      "e369ef22237e41989bd8ff0f618146f6",
      "4c96e2ab39c74ff6be2e1c00f5a5041b",
      "aad72ac1a14349d7af0593ded98deafd",
      "9180f9f83db2476e8c66d2d85880fc60",
      "ea0ba5ec533c4f1c85a93636c3d3838c",
      "f505a4942dd24b9599c0aaeca9f6e738",
      "55f1137cdfc84aa380a761bd4dd6f95d",
      "34c6e2eca6a84b7a966f60ae732cbcaa",
      "fb420fdbbc0748e7bfb067c7fd79c81e",
      "7cf0730cea6c44dea0a9526bb3c9abd9",
      "e0c85de17cbb41d1b33995ecd90c46fe",
      "b8ce9ad0303143568f87542d90226a46",
      "cf8a3148b7d04b90955d3da4edb69adb",
      "ff0fad8ad81847a4ab4529b2f7ab372e",
      "57244ef4ea4140efa2ac5587452f3bda",
      "15517a901d8b43b399034a97756388a8",
      "4fd675ee3e4b4260b0a0c64f5994a9be",
      "1be9319d79fd48559533e8388c763a5a",
      "e4901e64d8a444898f5dc08c338189fb",
      "4055e61b146e4e6f990f7003228710a6"
     ]
    },
    "id": "LckRxq-V1ATz",
    "outputId": "3cb5aa1e-8646-48ac-cd0f-7cebe91fa420"
   },
   "outputs": [],
   "source": [
    "# And we create a `fill-mask` pipeline for the 1760-1850 model. To do so, you\n",
    "# just need to add the path to the `model` argument. It is very important that\n",
    "# you know (1) which is the tokenizer that was used to train the model and (2)\n",
    "# on which task the model was fine-tuned, in this case `fill-mask`: this info\n",
    "# is usually given in the description of the model.\n",
    "unmasker_1760_1850 = pipeline('fill-mask',\n",
    "                              model='bert_1760_1850',\n",
    "                              tokenizer='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf2905",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_rzisZfnGkN",
    "outputId": "9397232a-25a6-4689-f854-80da7499ce0e"
   },
   "outputs": [],
   "source": [
    "# We can now use them to predict a mask in a sentence as well:\n",
    "outputs = unmasker_1760_1850(\"\"\"The [MASK] is guarded by guards.\"\"\")\n",
    "\n",
    "# Let's print the results in an easier-to-read format:\n",
    "for one_output in outputs:\n",
    "    print(\"Prediction:\", one_output['token_str'])\n",
    "    print(\"Score:     \", round(one_output['score'],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172d869",
   "metadata": {
    "id": "uSaj5vRG1cx-"
   },
   "source": [
    "### 2.6. Exercise\n",
    "\n",
    "Create a pipeline for the 1890-1900 model as well and try different sentences  with both the 1760-1850 and the 1890-1900 models. Do language models trained on data from different periods make different predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b6c0e",
   "metadata": {
    "id": "1s9ORTYrPKkB"
   },
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83953587",
   "metadata": {
    "id": "W2Ck9C8HZX6X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4254e05e",
   "metadata": {
    "id": "JzUAk_78av8c"
   },
   "source": [
    "### 2.7. The feature extraction pipeline\n",
    "\n",
    "Here we will see how to get vectors for words in context.\n",
    "\n",
    "Similarly to what we did with word2vec, we may also want to have access to the vector of a certain word. However, unlike with word2vec, the vector of a word will depend on the context in which the word occurs. This means that we can't just ask for the vector of the word \"apple\", for example: we will need to ask for the vector of the word \"apple\" given a certain context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a69793",
   "metadata": {
    "id": "Oh1tR1gk7FiO"
   },
   "source": [
    "We first import the following two libraries, which will help us work with vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d410e5d",
   "metadata": {
    "id": "0qGhhLcKbLct"
   },
   "outputs": [],
   "source": [
    "import numpy as np # python library used for working with vectors\n",
    "from scipy import spatial # package to help compute distance or similarity between vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0466616",
   "metadata": {
    "id": "yaiUaFkM8Epq"
   },
   "source": [
    "The pipeline task to obtain the vectors for tokens in a sequence is `feature-extraction`. As you can see, creating this pipeline is very similar to how creating the `fill-mask` pipeline.\n",
    "\n",
    "We will store the pipeline in a variable called `nlp_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e000779",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-XAVDugazIe",
    "outputId": "2f246551-6185-4fa8-b99b-08af2847cbdd"
   },
   "outputs": [],
   "source": [
    "nlp_features = pipeline(\"feature-extraction\",\n",
    "                    model='distilbert-base-uncased',\n",
    "                    tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ece87a",
   "metadata": {
    "id": "6Lysu1Dm_Weh"
   },
   "source": [
    "Given a sentence with a masked element, the pipeline tokenizes the input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a45a21",
   "metadata": {
    "id": "xZV_uhZ2-G1V"
   },
   "outputs": [],
   "source": [
    "sentence = \"They were told that the [MASK] stopped working.\"\n",
    "\n",
    "output = nlp_features(sentence)\n",
    "output_vectors = np.squeeze(output) # This removes single-dimensional entries (i.e. for vector readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ed195",
   "metadata": {
    "id": "B11uYzovCe8z"
   },
   "source": [
    "Let's inspect the output. First of all, let's print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06156d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkcTRDTeCoN0",
    "outputId": "c9a2dfbe-2f49-4718-d9b2-ed877e154a60"
   },
   "outputs": [],
   "source": [
    "print(output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30490918",
   "metadata": {
    "id": "3SCm6NODCjon"
   },
   "source": [
    "This is an array (a list of vectors). Let's see its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4544ca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xNx4rwN_Qkk",
    "outputId": "0b95c6d9-055b-4ca5-ed80-1d09def551b3"
   },
   "outputs": [],
   "source": [
    "print(output_vectors.shape) # Print the shape of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc119c3",
   "metadata": {
    "id": "pkrr6zpPEGI5"
   },
   "source": [
    "This means that we have an arrray (in other words a matrix, a table) that has 11 vectors of length 768 (or, in other words, 11 rows with 768 columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc164f",
   "metadata": {
    "id": "FIrLMeHlG0gG"
   },
   "source": [
    "**Question:** 11 vectors? Why 11?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced2e05",
   "metadata": {
    "id": "uuGWf0R_LV8G"
   },
   "source": [
    "Let's see how the sentence is tokenized (we've seen how above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f15e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeDbMIQFLY-E",
    "outputId": "e2f5d37d-3e2a-418a-b0fc-5b224d64e275"
   },
   "outputs": [],
   "source": [
    "# Load the **SAME** tokenizer used in the pipeline:\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Encode the sentence into a sequence of vocabulary IDs\n",
    "encoded_seq = our_tokenizer.encode(sentence)\n",
    "print(encoded_seq)\n",
    "\n",
    "# And get the tokens given the vocabulary IDs\n",
    "tokens = our_tokenizer.convert_ids_to_tokens(encoded_seq)\n",
    "print(tokens)\n",
    "\n",
    "# And print the length of the tokenized sequence:\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9cd93",
   "metadata": {
    "id": "IrgIvkB0G35O"
   },
   "source": [
    "As you can see, the input sentence has been tokenized into 11 tokens. So what we have in the above array is 11 vectors (each one representing a word in the context of the sentence, **keeping the order of tokens**, i.e. the first vector will correspond to the special token `[CLS]`, the second vector to the token `the`, and so on until the last vector, which corresponds to the special token `[SEP]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61553355",
   "metadata": {
    "id": "g60E3KzsOGzT"
   },
   "source": [
    "How do we get the vector of the `[MASK]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d0c31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0ATT1dKOZNw",
    "outputId": "8dbe0fc3-eeb3-43e0-9c2c-44e70a1103cb"
   },
   "outputs": [],
   "source": [
    "print(tokens[6]) # The [MASK] is the 6th element in the tokenized sentence (we start counting from zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c1ada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNMhJtHgIkFj",
    "outputId": "6484e968-a6aa-470a-fb0c-e64005baf543"
   },
   "outputs": [],
   "source": [
    "print(output_vectors[6]) # So the 6th vector in output_vectors is the vector of the [MASK] in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79358a",
   "metadata": {
    "id": "pm-7RWVSV4Kl"
   },
   "source": [
    "#### Compute the similarity between words in contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868693cb",
   "metadata": {
    "id": "YK7qTFlbWFDt"
   },
   "source": [
    "The following function (`get_embedding`) gets us a vector for a token in a sentence. It needs, as input:\n",
    "* `sentence`: the sentence where the target token appears.\n",
    "* `target_token`: the token for which we want to get a vector.\n",
    "* `nlp_features`: the `feature-extraction` pipeline.\n",
    "* `tokenizer`: the same tokenizer used by the `feature-extraction` pipeline.\n",
    "\n",
    "The function prints the list of encoded tokens in the sentence, the list of tokens, and the position of the target token in the sentence. The output of the function is the vector representing the target token in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e664c",
   "metadata": {
    "id": "8I7gJiZeIkHy"
   },
   "outputs": [],
   "source": [
    "def get_embedding(sentence, target_token, nlp_features, tokenizer):\n",
    "    encoded_seq = tokenizer.encode(sentence) # Tokenize and encode tokens into vocabulary IDs\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_seq) # Get the tokens corresponding to the IDs.\n",
    "    target_id_in_sentence = tokens.index(target_token) # Find the position of the target token in the sentence.\n",
    "    output = nlp_features(sentence) # Use the feature-extraction pipeline to convert the sentence into an array.\n",
    "    output = np.squeeze(output) # Squeeze the output from feature-extraction for readability.\n",
    "    print(\"Encoded tokens in sentence:\", encoded_seq)\n",
    "    print(\"Tokens in sentence:\", tokens)\n",
    "    print(\"Position of target id:\", target_id_in_sentence)\n",
    "    return output[target_id_in_sentence] # Return nth vector in the array (where n is the position of the target token in the sentence.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436d305",
   "metadata": {
    "id": "ax3Cb455ZFAz"
   },
   "source": [
    "We can call the function for different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15698ae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttSLaFg4IkLT",
    "outputId": "69c8138f-7fd8-4fed-ae5c-6d416aff448a"
   },
   "outputs": [],
   "source": [
    "# Get the embedding for [MASK] in the following sentence:\n",
    "sentence_1 = \"They were told that the [MASK] stopped working.\"\n",
    "target_token_1 = \"[MASK]\"\n",
    "\n",
    "tok_embedding_1 = get_embedding(sentence_1, target_token_1, nlp_features, our_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113b5df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x72VSRWIkNX",
    "outputId": "8c55f504-08e7-4654-ea57-5b335f6de12d"
   },
   "outputs": [],
   "source": [
    "# Get the embedding for [MASK] in the following sentence:\n",
    "sentence_2 = \"The [MASK] worked in the factory until dawn.\"\n",
    "target_token_2 = \"[MASK]\"\n",
    "\n",
    "tok_embedding_2 = get_embedding(sentence_2, target_token_2, nlp_features, our_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1989d4b",
   "metadata": {
    "id": "FwsS6nBoZVWT"
   },
   "source": [
    "We can now get the cosine similarity between any pair of vectors using the following (where `tok_embedding_1` and `tok_embedding_2` are the two vectors we want to compare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31edbc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lLRJju2IkQH",
    "outputId": "65933f5c-c864-450a-bf9d-c08ff1c05e68"
   },
   "outputs": [],
   "source": [
    "print(1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ad52b",
   "metadata": {
    "id": "8-F30T5CZtxA"
   },
   "source": [
    "Try to find the similarity between words in different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33860d0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPRp_v1fa3qv",
    "outputId": "1cb65cf5-84b6-45c4-ef62-a25f80f25ce4"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "sentence_1 = \"I would like to eat an apple.\"\n",
    "target_token_1 = \"apple\"\n",
    "tok_embedding_1 = get_embedding(sentence_1, target_token_1, nlp_features, our_tokenizer)\n",
    "\n",
    "# Example 2\n",
    "sentence_2 = \"I work with an apple macbook.\"\n",
    "target_token_2 = \"apple\"\n",
    "tok_embedding_2 = get_embedding(sentence_2, target_token_2, nlp_features, our_tokenizer)\n",
    "\n",
    "# Example 3\n",
    "sentence_3 = \"I made apples in the oven.\"\n",
    "target_token_3 = \"apples\"\n",
    "tok_embedding_3 = get_embedding(sentence_3, target_token_3, nlp_features, our_tokenizer)\n",
    "\n",
    "# Example 4\n",
    "sentence_4 = \"My apple device crashed and I had to restart it.\"\n",
    "target_token_4 = \"apple\"\n",
    "tok_embedding_4 = get_embedding(sentence_4, target_token_4, nlp_features, our_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69652e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8j9wi0j6bYO_",
    "outputId": "f898d3e6-043b-4990-dde7-2ab4ff290c57"
   },
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "print(\"Similarity sent1 and sent2:\", 1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_2))\n",
    "print(\"Similarity sent1 and sent3:\", 1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_3))\n",
    "print(\"Similarity sent1 and sent4:\", 1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_4))\n",
    "print(\"Similarity sent2 and sent4:\", 1 - spatial.distance.cosine(tok_embedding_2, tok_embedding_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874098a7",
   "metadata": {
    "id": "sYgcioF-_q9q"
   },
   "source": [
    "### 2.8. Exercise\n",
    "\n",
    "Create two `feature-extraction` pipelines, one for the  1760-1850 model, and one for the 1890-1900 model. Find whether the cosine similarity between words in sequences change depending on which BERT model you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d3372",
   "metadata": {
    "id": "N_KO4R2cARaX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5953065",
   "metadata": {
    "id": "Mf_6OjqOAXtI"
   },
   "source": [
    "##### Example solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f5563",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBV5kTKRAaR8",
    "outputId": "e8fa1a89-3810-49ea-b67c-e407986db77a"
   },
   "outputs": [],
   "source": [
    "# Create the pipeline for the 1760_1850 model, store it in a variable:\n",
    "nlp_features_1760_1850 = pipeline(\"feature-extraction\",\n",
    "                                  model='bert_1760_1850',\n",
    "                                  tokenizer='bert-base-uncased')\n",
    "\n",
    "# Create the pipeline for the 1890_1900 model, store it in a different variable:\n",
    "nlp_features_1890_1900 = pipeline(\"feature-extraction\",\n",
    "                                  model='bert_1890_1900',\n",
    "                                  tokenizer='bert-base-uncased')\n",
    "\n",
    "# Load the tokenizer of the BERT model (both models use the same tokenizer, so \n",
    "# we only need to do this once):\n",
    "our_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e942e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbIwSbgoBOSb",
    "outputId": "cff1a258-9946-4569-e039-c76ccd9146ca"
   },
   "outputs": [],
   "source": [
    "##### BERT MODEL 1760-1850\n",
    "\n",
    "# SENTENCE 1\n",
    "# Get the embedding for \"machines\" in the following sentence:\n",
    "sentence_1 = \"They were told that the machines stopped working.\"\n",
    "target_token_1 = \"machines\"\n",
    "tok_embedding_1 = get_embedding(sentence_1, target_token_1, nlp_features_1760_1850, our_tokenizer)\n",
    "\n",
    "# SENTENCE 2\n",
    "# Get the embedding for \"children\" in the following sentence:\n",
    "sentence_2 = \"The children worked in the factory until dawn.\"\n",
    "target_token_2 = \"children\"\n",
    "tok_embedding_2 = get_embedding(sentence_2, target_token_2, nlp_features_1760_1850, our_tokenizer)\n",
    "\n",
    "# Get the cosine similarity between \"machines\" in the first sentences and \"children\"\n",
    "# in the second sentence:\n",
    "print(\"Similarity sent1 and sent2:\", 1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9549c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3li3iKjBR4_",
    "outputId": "c10038a3-defc-4166-ad44-2bb4dc815125"
   },
   "outputs": [],
   "source": [
    "##### BERT MODEL 1890-1900\n",
    "\n",
    "# SENTENCE 1\n",
    "# Get the embedding for \"machines\" in the following sentence:\n",
    "sentence_1 = \"They were told that the machines stopped working.\"\n",
    "target_token_1 = \"machines\"\n",
    "tok_embedding_1 = get_embedding(sentence_1, target_token_1, nlp_features_1890_1900, our_tokenizer)\n",
    "\n",
    "# SENTENCE 2\n",
    "# Get the embedding for \"children\" in the following sentence:\n",
    "sentence_2 = \"The children worked in the factory until dawn.\"\n",
    "target_token_2 = \"children\"\n",
    "tok_embedding_2 = get_embedding(sentence_2, target_token_2, nlp_features_1890_1900, our_tokenizer)\n",
    "\n",
    "# Get the cosine similarity between \"machines\" in the first sentences and \"children\"\n",
    "# in the second sentence:\n",
    "print(\"Similarity sent1 and sent2:\", 1 - spatial.distance.cosine(tok_embedding_1, tok_embedding_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b497d1c",
   "metadata": {
    "id": "RLnsKVzkX-EO"
   },
   "source": [
    "#### Useful links\n",
    "\n",
    "**On pipelines**\n",
    "\n",
    "https://huggingface.co/transformers/quicktour.html\n",
    "\n",
    "**On training or fine-tuning your own model**\n",
    "\n",
    "https://huggingface.co/transformers/training.html\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\n",
    "\n",
    "https://colab.research.google.com/gist/aditya-malte/2d4f896f471be9c38eb4d723a710768b/smallberta_pretraining.ipynb\n",
    "\n",
    "https://colab.research.google.com/drive/19jDqa5D5XfxPU6NQef17BC07xQdRnaKU?usp=sharing\n",
    "\n",
    "https://colab.research.google.com/drive/1r_eoi8CMea_a3YjWC1M4EmTqKMGVMbzQ?usp=sharing\n",
    "\n",
    "**On BERT and the humanities**\n",
    "\n",
    "https://melaniewalsh.github.io/BERT-for-Humanists/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aec66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
