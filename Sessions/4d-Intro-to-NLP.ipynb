{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1960381c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linguistic processing with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aacf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we've been seeing, first of all we need to import Spacy and\n",
    "load a trained pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1792f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Download a pipeline:\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Load English language pipeline and store in variable `nlp`:\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5879f28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence-level processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385368f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae23c71",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence to process:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops. They bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# To retrieve sentences from a `doc` object, use the `sents` attribute:\n",
    "print([sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189be56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Token-level processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc73726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c4cb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe6c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Access the tokens text, either in a for-loop:\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab03863",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a024fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# ... or with a list-comprehension:\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c6a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955fbf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Access the tokens lemmas:\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c91f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4d747",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Get the part-of-speech of each token:\n",
    "print([token.pos_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d1660",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If we want to get more than one layer or information, we need to have them as a\n",
    "# tuple (i.e. enclosed in parentheses). For example, we want to get the text and\n",
    "# part-of-speech of each token:\n",
    "print([(token.text, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50feb79a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# As we have already seeen in the past, list comprehensions can include conditions.\n",
    "# For example, get only the text of the tokens that are nouns:\n",
    "print([token.text for token in doc if token.pos_ == \"NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc78d7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Or get only the verbs:\n",
    "print([token.text for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcfc80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instead of printing the result, we can store it in a new variable:\n",
    "list_of_verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "print(list_of_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66476fa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "‚úèÔ∏è **Exercises:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a3af0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Print the verbatim text, lemma, and part-of-speech of each token.\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8060c06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define a new function that takes a sentence and an nlp object as input,\n",
    "# and returns the adjectives in the text (as a list). Then try it with the\n",
    "# following sentences:\n",
    "# \n",
    "# * \"It was a bright cold day in April, and the clocks were striking thirteen.\"\n",
    "# * \"April is the Cruellest Month.\"\n",
    "# * \"Twas brillig, and the slithy toves did gyre and gimble in the wabe; all mimsy were the borogoves, and the mome raths outgrabe.\"\n",
    "# * ... and any other sentence you'd like to try!\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d7512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "üëÄ **Suggested solutions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ebe94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 1:\n",
    "\n",
    "print([(token.text, token.lemma_, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49ca27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2:\n",
    "\n",
    "def get_adjectives(text, nlp):\n",
    "    processed_text = nlp(text)\n",
    "    list_of_adjectives = [token.text for token in processed_text if token.pos_ == \"ADJ\"]\n",
    "    return list_of_adjectives\n",
    "\n",
    "text = \"April is the cruellest month.\"\n",
    "get_adjectives(text, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ad412",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e538ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pro tip: you can visualize the dependency parsing like this:\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "txt = \"A trifling incident thus served to settle a victory.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b0904",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"A trifling incident thus served to settle a victory.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# The text, the dependency and the head token for each token in the doc, stored in `parsing`:\n",
    "parsing = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "print(parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d413212",
   "metadata": {
    "id": "MBCNxvkafQ8u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Processing text in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afdf32",
   "metadata": {
    "id": "0L0vltyv1q0S",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First let's have a look at our data [here](data/LwM-nlp-animacy-annotations-machines19thC.tsv).\n",
    "\n",
    "The first step before any preprocessing or analysis can even begin should be to make sure you understand your data. The decision on which preprocessing steps (and which analyses you are able to carry out!) will change depending on the characteristics of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d5635",
   "metadata": {
    "id": "5FkuP3ay6GqU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will be using **pandas** to work with tabular data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ffc6a",
   "metadata": {
    "id": "HIN461kJ6Iro",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9f05b",
   "metadata": {
    "id": "OoHd-2R_vqYy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e70cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "6LISqiBHskWq",
    "outputId": "f8d30a65-2841-4977-a3f1-e12590f6da6f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can read the tsv file using pandas library. The resulting object is called\n",
    "# a dataframe, which we store in variable `df`:\n",
    "df = pd.read_csv(\"data/LwM-nlp-animacy-annotations-machines19thC.tsv\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7693ba",
   "metadata": {
    "id": "fo8wd_tIwnL3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Have a look at your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4326a0",
   "metadata": {
    "id": "TXd2p2vCYjZB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's print the first rows of our dataframe:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e14b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Function that takes a text and an nlp pipeline, and\n",
    "    returns a list of the verbs in the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text that will be processed.\n",
    "        nlp: A spacy pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        A list of verbs in the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    processed = [x.text for x in doc if x.pos_ == \"VERB\"]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eaf584",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the process_text() function to a column in the dataframe:\n",
    "df['SentenceCtxt'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4569d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the process_text() function to a column and store it as a new column:\n",
    "df['processed'] = df['SentenceCtxt'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1a067",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check the first rows: a new column has appeared, with our processing!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f433a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Store the dataframe with the added column:\n",
    "df.to_csv(\"data/animacy_processed.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20380702",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "‚úèÔ∏è **Exercises:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244332a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the SentenceCtxt column in the animacy dataset, write a function that returns a\n",
    "# list of lemma forms instead of the words. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e0d18",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the SentenceCtxt column in the animacy dataset, write a function that returns a\n",
    "# list of named entities instead of the words. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5fb87d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the SentenceCtxt column in the animacy dataset, write a function \"count_tokens\"\n",
    "# and a function \"count_lemmas\", and add one column for the total number of tokens in\n",
    "# TextSnippet and another one for the number of unique lemmas. Tip: remember that you\n",
    "# can convert a `list` into a `set` to get rid of duplicates.\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07b496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2e91717858118300f159f516e8add62a50cea7986ea1e6059fd0b766f06d37e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
