{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae374187",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linguistic processing with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876d7fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we've been seeing, first of all we need to import Spacy and\n",
    "load a trained pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd26035",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Load English language pipeline and store in variable `nlp`:\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2966b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentence-level processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfde8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cc0b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence to process:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops. They bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# To retrieve sentences from a `doc` object, use the `sents` attribute:\n",
    "print([sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a54bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Token-level processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e02d96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bd18a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Access the tokens text, either in a for-loop:\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122a244",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# ... or with a list-comprehension:\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84591009",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356cea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Access the tokens lemmas:\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4e160",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4262b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"Mr. Smith and Mr. Jones went to the shops and they bought a notebook.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Get the part-of-speech of each token:\n",
    "print([token.pos_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163a419",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get the text and part-of-speech of each token:\n",
    "print([(token.text, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c2cf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get only the nouns:\n",
    "print([token.text for token in doc if token.pos_ == \"NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70decaef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get only the verbs:\n",
    "print([token.text for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4202f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09f885",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Given a sentence:\n",
    "txt = \"A trifling incident thus served to settle a victory.\"\n",
    "\n",
    "# Process the sentence using the trained pipeline that has been loaded before:\n",
    "doc = nlp(txt)\n",
    "\n",
    "# Verbatim token content:\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Syntactic dependency relation:\n",
    "print([token.dep_ for token in doc])\n",
    "\n",
    "# The head of a dependency relation:\n",
    "print([token.head.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43822a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pro tip: you can visualize the dependency parsing like this:\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "txt = \"A trifling incident thus served to settle a victory.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 160})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a86e42",
   "metadata": {
    "id": "MBCNxvkafQ8u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Processing text in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69bda5",
   "metadata": {
    "id": "0L0vltyv1q0S",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First let's have a look at our data [here](XXXXXXX).\n",
    "\n",
    "The first step before any preprocessing or analysis can even begin should be to make sure you understand your data. The decision on which preprocessing steps (and which analyses you are able to carry out!) will change depending on the characteristics of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147a48d",
   "metadata": {
    "id": "5FkuP3ay6GqU",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will be using **pandas** to work with tabular data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90935f",
   "metadata": {
    "id": "HIN461kJ6Iro",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a5339",
   "metadata": {
    "id": "OoHd-2R_vqYy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33bfa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "6LISqiBHskWq",
    "outputId": "f8d30a65-2841-4977-a3f1-e12590f6da6f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can read the tsv file using pandas library. The resulting object is called\n",
    "# a dataframe, which we store in variable `df`:\n",
    "df = pd.read_csv(\"data/animacy.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167baa90",
   "metadata": {
    "id": "fo8wd_tIwnL3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Have a look at your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b972c1f",
   "metadata": {
    "id": "TXd2p2vCYjZB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's print the first rows of our dataframe:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f0e04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(text, nlp):\n",
    "    \"\"\"\n",
    "    Function that takes a text and an nlp pipeline, and\n",
    "    returns a list of the verbs in the text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text that will be processed.\n",
    "        nlp: A spacy pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        A list of verbs in the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    processed = [x.text for x in doc if x.pos_ == \"VERB\"]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f115efc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load a Spacy pipeline:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply the process_text() function to the 'TextSnippet' column:\n",
    "df['processed'] = df['TextSnippet'].apply(lambda x: process_text(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c1b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281dca6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Store the dataframe with the added column:\n",
    "df.to_csv(\"data/animacy_processed.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dd828",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "✏️ **Exercises:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e91be4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the TextSnippet column in the animacy dataset, write a function that returns a\n",
    "# list of lemma forms instead of the words. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eefa29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the TextSnippet column in the animacy dataset, write a function that returns a\n",
    "# list of named entities instead of the words. Type your code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16294dbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using the TextSnippet column in the animacy dataset, write a function \"count_tokens\"\n",
    "# and a function \"count_lemmas\", and add one column for the total number of tokens in\n",
    "# TextSnippet and another one for the number of unique lemmas. Tip: remember that you\n",
    "# can convert a `list` into a `set` to get rid of duplicates.\n",
    "# \n",
    "# Type your code here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
